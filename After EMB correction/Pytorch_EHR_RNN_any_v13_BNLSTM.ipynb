{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is playing with pytorch framework for EHR modeling. In general, a patient's health record can be represented as a sequence of visits. Each visit has certain features, and can be represented as a list of medical codes.\n",
    "\n",
    "For simplicity, we are starting with the data structure that a patient's health record is a list of list, following the line of work from Jimeng Sun's lab. We will use codes from Ed Choi to manipulate the data. \n",
    "\n",
    "The core model is an RNN , either LSTM, GRU or Vanilla RNN.\n",
    "\n",
    "# todos:\n",
    "* None for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import sklearn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "# for windows only    \n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import bnlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "#%env CUDA_VISIBLE_DEVICES=1\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29912 4272 8545\n"
     ]
    }
   ],
   "source": [
    "train_sl = pickle.load(open('pdata_3hosp/h143_train', 'rb'), encoding='bytes')\n",
    "valid_sl = pickle.load(open('pdata_3hosp/h143_valid', 'rb'), encoding='bytes')\n",
    "test_sl =  pickle.load(open('pdata_3hosp/h143_test', 'rb'), encoding='bytes')\n",
    "#test_sh_143 = pickle.load(open('pdata_3hosp/h143_test_sh', 'rb'), encoding='bytes')\n",
    "#test_l_143 = pickle.load(open('pdata_3hosp/h143_test_lng', 'rb'), encoding='bytes')\n",
    "print (len(train_sl),len(valid_sl),len(test_sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_118 = pickle.load(open('pdata_3hosp/h118_m', 'rb'), encoding='bytes')\n",
    "test_104 = pickle.load(open('pdata_3hosp/h104_m', 'rb'), encoding='bytes')\n",
    "test_3hosp = pickle.load(open('pdata_3hosp/h3hs_m', 'rb'), encoding='bytes')\n",
    "#test_sh_118 = pickle.load(open('pdata_3hosp/h118_test_sh', 'rb'), encoding='bytes')\n",
    "#test_l_118 = pickle.load(open('pdata_3hosp/h118_test_lng', 'rb'), encoding='bytes')\n",
    "#test_sh_104 = pickle.load(open('pdata_3hosp/h104_test_sh', 'rb'), encoding='bytes')\n",
    "#test_l_104 = pickle.load(open('pdata_3hosp/h104_test_lng', 'rb'), encoding='bytes')\n",
    "#test_sh_3hosp = pickle.load(open('pdata_3hosp/h3hs_test_sh', 'rb'), encoding='bytes')\n",
    "#test_l_3hosp = pickle.load(open('pdata_3hosp/h3hs_test_lng', 'rb'), encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom collections import OrderedDict\\ndef load_params(model):\\n    params = OrderedDict()\\n    weights = np.load(model)\\n    for k,v in weights.iteritems():\\n        params[k] = v\\n    return params\\n\\nparam= load_params(\"/data/projects/cerner_dev/cl2_hosp_retain_out/h50_cl2_all_hosp_143_cl2m.out.0.npz\")\\nemb_pretrain = param[\\'W_emb\\']\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from collections import OrderedDict\n",
    "def load_params(model):\n",
    "    params = OrderedDict()\n",
    "    weights = np.load(model)\n",
    "    for k,v in weights.iteritems():\n",
    "        params[k] = v\n",
    "    return params\n",
    "\n",
    "param= load_params(\"/data/projects/cerner_dev/cl2_hosp_retain_out/h50_cl2_all_hosp_143_cl2m.out.0.npz\")\n",
    "emb_pretrain = param['W_emb']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_RNN(nn.Module):\n",
    "    def __init__(self, input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='BNLSTM',bi=False , preTrainEmb=''):\n",
    "        super(EHR_RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cell_type = cell_type\n",
    "        self.preTrainEmb=preTrainEmb\n",
    "        if bi: self.bi=2 \n",
    "        else: self.bi=1\n",
    "              \n",
    "        if len(self.preTrainEmb)>0:\n",
    "            emb_t= torch.FloatTensor(np.asmatrix(self.preTrainEmb))\n",
    "            self.embed= nn.Embedding.from_pretrained(emb_t)#,freeze=False)  \n",
    "        else:\n",
    "            self.embed= nn.Embedding(input_size, self.embed_dim,padding_idx=0)\n",
    "        \n",
    "        if self.cell_type == \"GRU\":\n",
    "            cell = nn.GRU\n",
    "        elif self.cell_type == \"RNN\":\n",
    "            cell = nn.RNN\n",
    "        elif self.cell_type == \"LSTM\":\n",
    "            cell = nn.LSTM\n",
    "        elif self.cell_type == \"BNLSTM\":\n",
    "            cell = bnlstm.LSTM    \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "      \n",
    "        if self.cell_type == \"BNLSTM\":\n",
    "            self.rnn_c = bnlstm.LSTM (bnlstm.BNLSTMCell, self.embed_dim, hidden_size,num_layers=n_layers,use_bias=False, batch_first=True,dropout= dropout_r,max_length=30)\n",
    "        else:\n",
    "            self.rnn_c = cell(self.embed_dim, hidden_size,num_layers=n_layers, dropout= dropout_r , bidirectional=bi , batch_first=True )\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_size*self.bi,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def EmbedPatient_MB(self, input): # x is a ehr_seq_tensor\n",
    "        \n",
    "        mb=[]\n",
    "        lbt=[]\n",
    "        seq_l=[]\n",
    "        self.bsize=len(input)\n",
    "        lp= len(max(input, key=lambda xmb: len(xmb[1]))[1])\n",
    "        self.max_len_bn=lp\n",
    "        #print (lp)\n",
    "        llv=0\n",
    "        for x in input:\n",
    "            lv= len(max(x[1], key=lambda xmb: len(xmb)))\n",
    "            if llv< lv:\n",
    "                llv=lv\n",
    "        #print (llv)\n",
    "        for pt in input:\n",
    "            label, ehr_seq_l = pt\n",
    "            lpx=len(ehr_seq_l)\n",
    "            seq_l.append(lpx)\n",
    "            label_tensor = Variable(torch.FloatTensor([[float(label)]]))\n",
    "            if use_cuda:\n",
    "                label_tensor = label_tensor.cuda()\n",
    "            lbt.append(label_tensor)\n",
    "            ml=(len(max(ehr_seq_l, key=len)))\n",
    "            ehr_seq_tl=[]\n",
    "            for ehr_seq in ehr_seq_l: \n",
    "                pd=(0, llv-len(ehr_seq))\n",
    "                result = F.pad(torch.from_numpy(np.asarray(ehr_seq,dtype=int)).type(torch.cuda.LongTensor),pd,\"constant\", 0)\n",
    "                if use_cuda:\n",
    "                    result.cuda()\n",
    "                ehr_seq_tl.append(result)\n",
    "            ehr_seq_t= Variable(torch.stack(ehr_seq_tl,0))     \n",
    "            lpp= lp-lpx\n",
    "            zp= nn.ZeroPad2d((0,0,lpp,0))\n",
    "            ehr_seq_t= zp(ehr_seq_t)\n",
    "            mb.append(ehr_seq_t)\n",
    "                \n",
    "        mb_t= Variable(torch.stack(mb,0)) \n",
    "        if use_cuda:\n",
    "            mb_t.cuda()\n",
    "        embedded = self.embed(mb_t)\n",
    "        embedded = torch.sum(embedded, dim=2) \n",
    "        lbt_t= Variable(torch.stack(lbt,0)) \n",
    "        return embedded, lbt_t,seq_l\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        h_0 = Variable(torch.rand(self.n_layers*self.bi,self.bsize, self.hidden_size))\n",
    "        \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            result = (h_0,h_0)\n",
    "        else: \n",
    "            result = h_0\n",
    "            \n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x_in , lt ,x_lens = self.EmbedPatient_MB(input)\n",
    "        #x_inp = nn.utils.rnn.pack_padded_sequence(x_in,x_lens,batch_first=True)\n",
    "        #h_0 = self.init_hidden()\n",
    "        output, hidden = self.rnn_c(x_in)#,h_0) \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden=hidden[0]\n",
    "        if self.bi==2:\n",
    "            output = self.sigmoid(self.out(torch.cat((hidden[-2],hidden[-1]),1)))\n",
    "        else:\n",
    "            output = self.sigmoid(self.out(hidden[-1]))\n",
    "        return output.squeeze(), lt.squeeze()\n",
    "\n",
    "# GRU Epoch 2: Valid_auc :  0.8323451289474217  ,& Test_auc :  0.8432979973495972 with Random hidden\n",
    "## bestValidAuc 0.821507 has a TestAuc of 0.837944 at epoch 2 with default h_0\n",
    "## with adagrad 0.8213349966422361  ,& Test_auc :  0.8434773468886594 \n",
    "#LSTM bestValidAuc 0.821653 has a TestAuc of 0.838208 at epoch 7 \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgindybekhet/test/Pytorch_Code_tst/bnlstm.py:172: UserWarning:\n",
      "\n",
      "nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
      "\n",
      "/home/lgindybekhet/test/Pytorch_Code_tst/bnlstm.py:179: UserWarning:\n",
      "\n",
      "nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb_pretrain=''                       \n",
    "model = EHR_RNN(input_size=15816, hidden_size=64 ,embed_dim=256, dropout_r=0, cell_type='BNLSTM',bi=False , n_layers=1, preTrainEmb=emb_pretrain)\n",
    "bmodel_pth = './best_model_BNLSTM.pth'\n",
    "bmodel_st = './best_model_BNLSTM.st'\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (tmodel,mini_batch, criterion, optimizer):  \n",
    "    tmodel.train()\n",
    "    model.zero_grad()\n",
    "    output , label_tensor = tmodel(mini_batch)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train(tmodel,dataset,batch_size,learning_rate = 0.05, l2=3e-04,epsl=1e-06 ):\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)#, weight_decay=l2)\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    #optimizer = optim.ASGD(model.parameters(), lr=learning_rate, weight_decay=l2 )\n",
    "    #optimizer = optim.SparseAdam (model.parameters(),lr=learning_rate) #'''lr=learning_rate,''' \n",
    "    optimizer = optim.Adagrad (tmodel.parameters(),lr=learning_rate, weight_decay=l2) #'''lr=learning_rate,''' \n",
    "    #optimizer = optim.Adamax(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=l2 ,eps=epsl) ### Beta defaults (0.9, 0.999)\n",
    "    #optimizer = optim.RMSprop (model.parameters(),lr=learning_rate, weight_decay=l2 ,eps=epsl)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=learning_rate)\n",
    "    dataset.sort(key=lambda pt:len(pt[1]),reverse=True) \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    #print('number of Batches',n_batches)\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(tmodel,batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "            # Print iter number, loss, name and guess\n",
    "            #if iter % print_every == 0:\n",
    "               #print('%d %d%% (%s) %.4f ' % ( iter, iter/ n_batches * 100, timeSince(start), loss))\n",
    "\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "\n",
    "    test_model.eval()\n",
    "    dataset.sort(key=lambda pt:len(pt[1]),reverse=True) \n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "    \n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = test_model(batch)\n",
    "            y_hat.extend(output.cpu().data.view(-1).numpy())\n",
    "            labelVec.extend(label_t.cpu().data.view(-1).numpy())\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train_auc : 0.7957729127575853  , Valid_auc :  0.7759308710460213  ,& Test_auc :  0.7953852914885522  Avg Loss:  0.3224477071563403 Train Time (0m 52s) Eval Time (1m 4s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgindybekhet/.local/lib/python3.5/site-packages/torch/serialization.py:193: UserWarning:\n",
      "\n",
      "Couldn't retrieve source code for container of type EHR_RNN. It won't be checked for correctness upon loading.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train_auc : 0.8209505776747933  , Valid_auc :  0.7952438873124612  ,& Test_auc :  0.8178465436937085  Avg Loss:  0.30958301822344464 Train Time (0m 58s) Eval Time (1m 16s)\n",
      "Epoch  2  Train_auc : 0.8316153968479696  , Valid_auc :  0.806575315818319  ,& Test_auc :  0.8306146263832282  Avg Loss:  0.30143493572870894 Train Time (0m 51s) Eval Time (1m 18s)\n",
      "Epoch  3  Train_auc : 0.835963151678813  , Valid_auc :  0.8100120376790829  ,& Test_auc :  0.8274683269257912  Avg Loss:  0.30058792605996126 Train Time (0m 52s) Eval Time (1m 18s)\n",
      "Epoch  4  Train_auc : 0.825542602961168  , Valid_auc :  0.7939149887409724  ,& Test_auc :  0.8147599367667098  Avg Loss:  0.30079938977956766 Train Time (0m 52s) Eval Time (1m 4s)\n",
      "Epoch  5  Train_auc : 0.8459661033890722  , Valid_auc :  0.8140522827603278  ,& Test_auc :  0.8339537367931976  Avg Loss:  0.29552135358254117 Train Time (0m 51s) Eval Time (1m 3s)\n",
      "Epoch  6  Train_auc : 0.8384182172677269  , Valid_auc :  0.8066438917931225  ,& Test_auc :  0.8254060111873863  Avg Loss:  0.29667350505789114 Train Time (0m 51s) Eval Time (1m 3s)\n",
      "Epoch  7  Train_auc : 0.8475922435063655  , Valid_auc :  0.8138910882378815  ,& Test_auc :  0.8313605790528985  Avg Loss:  0.2952209454526503 Train Time (0m 51s) Eval Time (1m 3s)\n",
      "Epoch  8  Train_auc : 0.8472875389599293  , Valid_auc :  0.8118698728530392  ,& Test_auc :  0.8290204007041272  Avg Loss:  0.29277819658319154 Train Time (0m 51s) Eval Time (1m 4s)\n",
      "Epoch  9  Train_auc : 0.8495569329681795  , Valid_auc :  0.8133528624595443  ,& Test_auc :  0.8348648977190927  Avg Loss:  0.2929937178393205 Train Time (0m 51s) Eval Time (1m 4s)\n",
      "Epoch  10  Train_auc : 0.8482202501664072  , Valid_auc :  0.8096497598201615  ,& Test_auc :  0.8298814552576609  Avg Loss:  0.29255274335543313 Train Time (0m 51s) Eval Time (1m 4s)\n",
      "Epoch  11  Train_auc : 0.8431653556537715  , Valid_auc :  0.7982847263545395  ,& Test_auc :  0.823141774246929  Avg Loss:  0.2939765033125878 Train Time (0m 51s) Eval Time (1m 4s)\n",
      "Epoch  12  Train_auc : 0.8331240511292896  , Valid_auc :  0.7988562838815861  ,& Test_auc :  0.8203998610890909  Avg Loss:  0.2910916321973006 Train Time (0m 51s) Eval Time (1m 4s)\n",
      "Epoch  13  Train_auc : 0.8471100778370605  , Valid_auc :  0.8092159006648317  ,& Test_auc :  0.8304630155143472  Avg Loss:  0.29244885064661497 Train Time (0m 52s) Eval Time (1m 4s)\n",
      "Epoch  14  Train_auc : 0.8468663462094934  , Valid_auc :  0.8069845859786994  ,& Test_auc :  0.8303206508688594  Avg Loss:  0.2917924022177854 Train Time (0m 52s) Eval Time (1m 12s)\n",
      "Epoch  15  Train_auc : 0.8444841237594579  , Valid_auc :  0.8131140759974799  ,& Test_auc :  0.8291408055690547  Avg Loss:  0.291086300984025 Train Time (0m 52s) Eval Time (1m 3s)\n",
      "Epoch  16  Train_auc : 0.839663512712755  , Valid_auc :  0.8060887269293755  ,& Test_auc :  0.8235007452728003  Avg Loss:  0.29175252839922905 Train Time (0m 51s) Eval Time (1m 3s)\n",
      "Epoch  17  Train_auc : 0.8411972731762604  , Valid_auc :  0.8101480967844698  ,& Test_auc :  0.8284569929595448  Avg Loss:  0.29325923706094426 Train Time (0m 52s) Eval Time (1m 3s)\n",
      "Epoch  18  Train_auc : 0.8473586909258896  , Valid_auc :  0.8069045351395863  ,& Test_auc :  0.8248171763284716  Avg Loss:  0.2898799911141395 Train Time (0m 51s) Eval Time (1m 8s)\n",
      "bestValidAuc 0.814052 has a TestAuc of 0.833954 at epoch 5 \n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "batch_size=100\n",
    "current_loss_l=[]\n",
    "all_losses_l=[]\n",
    "train_auc_allep =[]\n",
    "valid_auc_allep =[]\n",
    "test_auc_allep=[]\n",
    "#tests_auc_allep=[]\n",
    "#testl_auc_allep=[]\n",
    "bestValidAuc = 0.0\n",
    "bestTestAuc = 0.0\n",
    "bestValidEpoch = 0\n",
    "\n",
    "for ep in range(epochs):\n",
    "    \n",
    "    #print (model.embed.weight.data[135] )\n",
    "    start = time.time()\n",
    "    current_loss_la,all_losses_la = run_model_train(model,train_sl,batch_size)\n",
    "    train_time = timeSince(start)\n",
    "    eval_start = time.time()\n",
    "    train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "    test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "    #test_sh_auc = calculate_auc(model,test_sh_143,batch_size)\n",
    "    #test_l_auc = calculate_auc(model,test_l_143,batch_size)\n",
    "    valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "    eval_time = timeSince(eval_start)\n",
    "    all_losses_l.append (all_losses_la)\n",
    "    avg_loss = np.mean(all_losses_la)\n",
    "    train_auc_allep.append(train_auc)\n",
    "    valid_auc_allep.append(valid_auc)\n",
    "    test_auc_allep.append(test_auc)\n",
    "    #testl_auc_allep.append(test_l_auc)\n",
    "    #tests_auc_allep.append(test_sh_auc)\n",
    "    current_loss_l.append(current_loss_la)\n",
    "    print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "    #print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\"less than 5 visits history :\",test_sh_auc,\" for more :\",test_l_auc ,\" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "    \n",
    "    if valid_auc > bestValidAuc: \n",
    "        bestValidAuc = valid_auc\n",
    "        bestValidEpoch = ep\n",
    "        bestTestAuc = test_auc\n",
    "        best_model = model\n",
    "        torch.save(best_model, bmodel_pth)\n",
    "        torch.save(best_model.state_dict(), bmodel_st)\n",
    "    if ep - bestValidEpoch >12: break\n",
    "            \n",
    "print ('bestValidAuc %f has a TestAuc of %f at epoch %d ' % (bestValidAuc, bestTestAuc, bestValidEpoch))\n",
    "torch.save(best_model, bmodel_pth)\n",
    "\n",
    "# BiGRU:bestValidAuc 0.779129 has a TestAuc of 0.794677 at epoch 8 \n",
    "#Test Auc for H104:  0.68506300670682 , H118:  0.6706375112328244  , combining together with H143 testset: 0.6858711757128325\n",
    "\n",
    "# GRU - ASGD  \n",
    "# Test Auc for H104:  0.6422552722859154 , H118:  0.6170000062045927  , combining together with H143 testset: 0.6347522649999163\n",
    "#bestValidAuc 0.690075 has a TestAuc of 0.698186 at epoch 6\n",
    "# Epoch  6  Train_auc : 0.7802418475435675  , Valid_auc :  0.6900751166474599  ,& Test_auc :  0.6981864076708845  Avg Loss:  0.31241230348745985 Train Time (1m 19s) Eval Time (2m 42s)\n",
    "\n",
    "\n",
    "# RNN \n",
    "#bestValidAuc 0.712566 has a TestAuc of 0.730539 at epoch 15\n",
    "#Test Auc for H104:  0.6530464804197255 , H118:  0.6440123151293581  , combining together with H143 testset: 0.6539902391808247\n",
    "# Epoch  15  Train_auc : 0.8200194776172784  , Valid_auc :  0.7125661239055848  ,& Test_auc :  0.7305390113122104  Avg Loss:  0.28983430554469425 Train Time (0m 51s) Eval Time (1m 8s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Auc for H104:  0.7760716066241817 , H118:  0.7413352136245663  , combining together with H143 testset: 0.7648787425324082\n"
     ]
    }
   ],
   "source": [
    "best_model.load_state_dict(torch.load(bmodel_st))\n",
    "best_model.eval()\n",
    "test_auc_118 = calculate_auc(best_model,test_118,batch_size)\n",
    "#test_auc_118_sh = calculate_auc(best_model,test_sh_118,batch_size)\n",
    "#test_auc_118_L = calculate_auc(best_model,test_l_118,batch_size)\n",
    "#print ('TestAuc Hosp118: ',test_auc_118 , 'pts with less than 5 visits: ',test_auc_118_sh, ' and those with longer history' ,test_auc_118_L )\n",
    "test_auc_104 = calculate_auc(best_model,test_104,batch_size)\n",
    "#test_auc_104_sh = calculate_auc(best_model,test_sh_104,batch_size)\n",
    "#test_auc_104_L = calculate_auc(best_model,test_l_104,batch_size)\n",
    "#print ('TestAuc Hosp104: ',test_auc_104 , 'pts with less than 5 visits: ',test_auc_104_sh, ' and those with longer history' ,test_auc_104_L )\n",
    "test_auc_3hosp = calculate_auc(best_model,test_3hosp,batch_size)\n",
    "#test_auc_3hosp_sh = calculate_auc(best_model,test_sh_3hosp,batch_size)\n",
    "#test_auc_3hosp_L = calculate_auc(best_model,test_l_3hosp,batch_size)\n",
    "#print ('TestAuc for 3 hospitals combined: ',test_auc_3hosp , 'pts with less than 5 visits: ',test_auc_3hosp_sh, ' and those with longer history' ,test_auc_3hosp_L )\n",
    "print ('Test Auc for H104: ',test_auc_104 , ', H118: ',test_auc_118,' , combining together with H143 testset:' ,test_auc_3hosp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8339537367931976"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sl_v = calculate_auc(best_model,test_sl,batch_size)\n",
    "test_sl_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~LailaRasmy/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('LailaRasmy','mzNHzVvwYjcZwBDZx3B7')\n",
    "\n",
    "train_auc_fg= go.Scatter(x= np.arange(epochs), y=train_auc_allep, name='train')\n",
    "test_auc_fg= go.Scatter(x= np.arange(epochs), y=test_auc_allep, name='test')\n",
    "#tests_auc_fg= go.Scatter(x= np.arange(epochs), y=tests_auc_allep, name='test<5')\n",
    "#testl_auc_fg= go.Scatter(x= np.arange(epochs), y=testl_auc_allep, name='test5+')\n",
    "\n",
    "valid_auc_fg= go.Scatter(x= np.arange(epochs), y=valid_auc_allep, name='valid')\n",
    "valid_max = max(valid_auc_allep)\n",
    "test_max = max(test_auc_allep)\n",
    "#data = [train_auc_fg,test_auc_fg,valid_auc_fg,tests_auc_fg,testl_auc_fg]#,valid_auc_allep,test_auc_allep] \n",
    "data = [train_auc_fg,test_auc_fg,valid_auc_fg]\n",
    "layout = go.Layout(xaxis=dict(dtick=1))\n",
    "layout.update(dict(annotations=[go.Annotation(text=\"Max Valid\", x=valid_auc_allep.index(valid_max), y=valid_max)]))\n",
    "#layout.update(dict(annotations=[go.Annotation(text=\"Max Test\", x=test_auc_allep.index(test_max), y=test_max)]))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='BiRNN_Auc')\n",
    "#url = py.plot(data, filename='some-data')  # gen. online plot\n",
    "#py.image.save_as(data, 'some-data.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
