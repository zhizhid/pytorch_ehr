{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(1)\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12586 1798 3596\n"
     ]
    }
   ],
   "source": [
    "train_sl= pickle.load(open('/data/projects/py_ehr_2/Data/Readm_h143_cl2_mde_tp_S.train', 'rb'), encoding='bytes')\n",
    "test_sl= pickle.load(open('/data/projects/py_ehr_2/Data/Readm_h143_cl2_mde_tp_S.test', 'rb'), encoding='bytes')\n",
    "valid_sl= pickle.load(open('/data/projects/py_ehr_2/Data/Readm_h143_cl2_mde_tp_S.valid', 'rb'), encoding='bytes')\n",
    "print (len(train_sl),len(valid_sl),len(test_sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13388336, 0, [[[0], [[6244, 2879, 265], [533, 92, 131, 509, 128, 68, 83, 30, 28, 58, 166, 8, 87, 88, 40, 101, 20, 113, 37, 120, 109, 54, 4, 44, 94, 17], []]], [[52], [[58, 2261], [], []]], [[7], [[397, 443, 28, 522, 18, 175, 120, 176, 516], [88, 37, 87, 40, 166, 94, 53, 92, 7, 30, 83, 824, 113, 131, 28], []]], [[9], [[443, 28, 3, 56], [], []]], [[27], [[443], [], []]], [[64], [[19, 132, 44, 610, 328, 443, 228, 671, 397, 3], [], []]], [[25], [[443], [], []]], [[70], [[28, 18, 443], [], []]], [[15], [[397, 218, 163, 18, 93, 1184, 1253, 31, 19, 231, 516, 632, 58], [7], []]], [[14], [[930, 2789, 1453], [], []]], [[13], [[443, 163], [], []]], [[52], [[886, 58, 163], [], []]]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_RNN(nn.Module):\n",
    "    def __init__(self, input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='LSTM',bi=False ,time=False, preTrainEmb=''):\n",
    "        ### need either to split the input_size, make it a list or a dictionary or take it out, the same for pretrained emb \n",
    "        super(EHR_RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cell_type = cell_type\n",
    "        self.preTrainEmb=preTrainEmb\n",
    "        self.time=time\n",
    "        \n",
    "        if bi: self.bi=2 \n",
    "        else: self.bi=1\n",
    "        \n",
    "        if self.time: self.in_size= (1*embed_dim)+1 \n",
    "        else: self.in_size=1*embed_dim\n",
    "            \n",
    "       # if len(self.preTrainEmb)>0:\n",
    "       #     emb_t= torch.FloatTensor(np.asmatrix(self.preTrainEmb))\n",
    "       #     self.embed= nn.Embedding.from_pretrained(emb_t)#,freeze=False)  \n",
    "       # else:\n",
    "       #    self.embed= nn.Embedding(input_size, self.embed_dim,padding_idx=0)#,scale_grad_by_freq=True)\n",
    "##########Replace above with separate embedding for each data category\n",
    "        self.embed_d= nn.Embedding(8655, self.embed_dim,padding_idx=0)#,scale_grad_by_freq=True)\n",
    "        self.embed_m= nn.Embedding(925, self.embed_dim,padding_idx=0)#,scale_grad_by_freq=True)\n",
    "        self.embed_o= nn.Embedding(3410, self.embed_dim,padding_idx=0)#,scale_grad_by_freq=True)\n",
    "    \n",
    "        if self.cell_type == \"GRU\":\n",
    "            cell = nn.GRU\n",
    "        elif self.cell_type == \"RNN\":\n",
    "            cell = nn.RNN\n",
    "        elif self.cell_type == \"LSTM\":\n",
    "            cell = nn.LSTM\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "          \n",
    "        self.rnn_c = cell(self.in_size, hidden_size,num_layers=n_layers, dropout= dropout_r , bidirectional=bi , batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size*self.bi,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def EmbedPatient_MB(self, input): # x is a ehr_seq_tensor\n",
    "\n",
    "        if use_cuda:\n",
    "            flt_typ=torch.cuda.FloatTensor\n",
    "            lnt_typ=torch.cuda.LongTensor\n",
    "        else: \n",
    "            lnt_typ=torch.LongTensor\n",
    "            flt_typ=torch.FloatTensor\n",
    "        #mb=[] #split to diff types\n",
    "        mbd=[]\n",
    "        mbm=[]\n",
    "        mbo=[]\n",
    "        \n",
    "        mtd=[]\n",
    "        lbt=[]\n",
    "        seq_l=[]\n",
    "        self.bsize=len(input) ## no of pts in minibatch\n",
    "        lp= len(max(input, key=lambda xmb: len(xmb[-1]))[-1]) ## maximum number of visits per patients in minibatch # this remains fine with whatever input format\n",
    "        \n",
    "        #llv=0 # split as well\n",
    "        llvd=0\n",
    "        llvm=0\n",
    "        llvo=0\n",
    "\n",
    "        for x in input:\n",
    "                lvd= len(max(x[-1], key=lambda xmb: len(xmb[1][0]))[1][0])\n",
    "                lvm= len(max(x[-1], key=lambda xmb: len(xmb[1][1]))[1][1])\n",
    "                lvo= len(max(x[-1], key=lambda xmb: len(xmb[1][2]))[1][2])\n",
    "                #print(lvd,lvm,lvo)\n",
    "                if llvd < lvd:  llvd=lvd     # max number of diagnosis codes per visit in minibatch        \n",
    "                if llvm < lvm:  llvm=lvm     # max number of medication codes per visit in minibatch        \n",
    "                if llvo < lvo:  llvo=lvo     # max number of demographics and other codes per visit in minibatch        \n",
    "\n",
    "        #print(llvd,llvm,llvo)        \n",
    "        #if llvd==0: llvd=1\n",
    "        #if llvm==0: llvm=1\n",
    "        \n",
    "            \n",
    "        '''\n",
    "        llv=0\n",
    "        for x in input:\n",
    "            lv= len(max(x[-1], key=lambda xmb: len(xmb[1]))[1])\n",
    "            if llv < lv:\n",
    "                llv=lv     # max number of codes per visit in minibatch        \n",
    "        '''\n",
    "        for pt in input:\n",
    "            sk,label,ehr_seq_l = pt\n",
    "            lpx=len(ehr_seq_l) ## no of visits in pt record\n",
    "            seq_l.append(lpx) \n",
    "            lbt.append(Variable(flt_typ([[float(label)]])))### check if code issue replace back to the above\n",
    "            \n",
    "            #ml=(len(max(ehr_seq_l, key=len))) # max number of codes per visit in pt record\n",
    "            time_dim=[]\n",
    "            #ehr_seq_tl=[]  ## split to diff types        \n",
    "            ehr_seq_tld=[]\n",
    "            ehr_seq_tlm=[]\n",
    "            ehr_seq_tlo=[]\n",
    "          \n",
    "            for ehr_seq in ehr_seq_l:\n",
    "                #pd=(0, (llv -len(ehr_seq[1])))\n",
    "                #result = F.pad(torch.from_numpy(np.asarray(ehr_seq[1],dtype=int)).type(lnt_typ),pd,\"constant\", 0)\n",
    "                #ehr_seq_tl.append(result)    ## split to diff types \n",
    "                pdd=(0, (llvd -len(ehr_seq[1][0])))\n",
    "                pdm=(0, (llvm -len(ehr_seq[1][1])))\n",
    "                pdo=(0, (llvo -len(ehr_seq[1][2])))\n",
    "                #print(pdd,pdm,pdo)\n",
    "                if len(ehr_seq[1][0])==0: resultd = F.pad(lnt_typ([0]),(0,llvd-1),\"constant\", 0)    \n",
    "                else: resultd = F.pad(torch.from_numpy(np.asarray(ehr_seq[1][0],dtype=int)).type(lnt_typ),pdd,\"constant\", 0)\n",
    "                if len(ehr_seq[1][1])==0: resultm = F.pad(lnt_typ([0]),(0,llvm-1),\"constant\", 0)     \n",
    "                else:resultm = F.pad(torch.from_numpy(np.asarray(ehr_seq[1][1],dtype=int)).type(lnt_typ),pdm,\"constant\", 0)\n",
    "                if len(ehr_seq[1][2])==0: resulto = F.pad(lnt_typ([0]),(0,llvo-1),\"constant\", 0)     \n",
    "                else: resulto = F.pad(torch.from_numpy(np.asarray(ehr_seq[1][2],dtype=int)).type(lnt_typ),pdo,\"constant\", 0)\n",
    "                ehr_seq_tld.append(resultd)\n",
    "                ehr_seq_tlm.append(resultm)\n",
    "                ehr_seq_tlo.append(resulto)\n",
    "\n",
    "                \n",
    "                if self.time:                 \n",
    "                    #time_dim.append(Variable(torch.from_numpy(np.asarray(ehr_seq[0],dtype=int)).type(flt_typ)))\n",
    "                    # use log time as RETAIN\n",
    "                    time_dim.append(Variable(torch.div(flt_typ([1.0]), torch.log(torch.from_numpy(np.asarray(ehr_seq[0],dtype=int)).type(flt_typ) + flt_typ([2.7183])))))\n",
    "                    \n",
    "            lpp= lp-lpx ## diff be max seq in minibatch and cnt of pt visits\n",
    "            zp= nn.ZeroPad2d((0,0,0,lpp)) ## (0,0,0,lpp) when use the pack padded seq and (0,0,lpp,0) otherwise\n",
    "  \n",
    "            #ehr_seq_t= Variable(torch.stack(ehr_seq_tl,0)) \n",
    "            #ehr_seq_t= zp(ehr_seq_t) ## zero pad the visits med codes  \n",
    "            #mb.append(ehr_seq_t) ## split to diff types\n",
    "            ehr_seq_td= Variable(torch.stack(ehr_seq_tld,0)) \n",
    "            ehr_seq_tm= Variable(torch.stack(ehr_seq_tlm,0)) \n",
    "            ehr_seq_to= Variable(torch.stack(ehr_seq_tlo,0)) \n",
    "            ehr_seq_td= zp(ehr_seq_td) ## zero pad the visits diag codes\n",
    "            ehr_seq_tm= zp(ehr_seq_tm) ## zero pad the visits med codes\n",
    "            ehr_seq_to= zp(ehr_seq_to) ## zero pad the visits dem&other codes\n",
    "            mbd.append(ehr_seq_td)\n",
    "            mbm.append(ehr_seq_tm)\n",
    "            mbo.append(ehr_seq_to)\n",
    "            \n",
    "            if self.time:\n",
    "                time_dim_v= Variable(torch.stack(time_dim,0))\n",
    "                time_dim_pv= zp(time_dim_v)## zero pad the visits time diff codes\n",
    "                mtd.append(time_dim_pv)\n",
    "\n",
    "            \n",
    "        #mb_t= Variable(torch.stack(mb,0)) \n",
    "        #if use_cuda:\n",
    "        #    mb_t.cuda()\n",
    "        #embedded = self.embed(mb_t)  ## Embedding for codes\n",
    "        #embedded = torch.sum(embedded, dim=2) #### split all the above\n",
    "        \n",
    "        mb_td= Variable(torch.stack(mbd,0))\n",
    "        mb_tm= Variable(torch.stack(mbm,0))\n",
    "        mb_to= Variable(torch.stack(mbo,0))\n",
    "        if use_cuda:\n",
    "            mb_td.cuda()\n",
    "            mb_tm.cuda()\n",
    "            mb_to.cuda()\n",
    "        embedded_d = torch.sum(self.embed_d(mb_td), dim=2) \n",
    "        embedded_m = torch.sum(self.embed_m(mb_tm), dim=2) \n",
    "        embedded_o = torch.sum(self.embed_o(mb_to), dim=2)      \n",
    "\n",
    "        #embedded=torch.cat((embedded_d,embedded_m,embedded_o),dim=2)## the concatination of above\n",
    "        #embedded=torch.cat((embedded_d,embedded_m),dim=2)## the concatination of above\n",
    "        embedded=embedded_d\n",
    "        lbt_t= Variable(torch.stack(lbt,0))\n",
    "        if self.time:\n",
    "            mtd_t= Variable(torch.stack(mtd,0))\n",
    "            if use_cuda: mtd_t.cuda()\n",
    "            out_emb= torch.cat((embedded,mtd_t),dim=2)\n",
    "        else:\n",
    "            out_emb= embedded\n",
    "        return out_emb, lbt_t,seq_l #,dem_emb\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        h_0 = Variable(torch.rand(self.n_layers*self.bi,self.bsize, self.hidden_size))\n",
    "        \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            result = (h_0,h_0)\n",
    "        else: \n",
    "            result = h_0\n",
    "            \n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x_in , lt ,x_lens = self.EmbedPatient_MB(input)\n",
    "        x_inp = nn.utils.rnn.pack_padded_sequence(x_in,x_lens,batch_first=True)   \n",
    "        #h_0= self.init_hidden()\n",
    "        output, hidden = self.rnn_c(x_inp)#,h_0) \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden=hidden[0]\n",
    "        if self.bi==2:\n",
    "            output = self.sigmoid(self.out(torch.cat((hidden[-2],hidden[-1]),1)))\n",
    "        else:\n",
    "            output = self.sigmoid(self.out(hidden[-1]))\n",
    "        return output.squeeze(), lt.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (tmodel,mini_batch, criterion, optimizer):  \n",
    "    tmodel.zero_grad()\n",
    "    output , label_tensor = tmodel(mini_batch)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train(tmodel,dataset,batch_size,optimizer):\n",
    "        \n",
    "    tmodel.train()\n",
    "    dataset.sort(key=lambda pt:len(pt[-1]),reverse=True) \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(tmodel,batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "    test_model.eval()\n",
    "    dataset.sort(key=lambda pt:len(pt[-1]),reverse=True) \n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "    \n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = test_model(batch)\n",
    "            y_hat.extend(output.cpu().data.view(-1).numpy())\n",
    "            labelVec.extend(label_t.cpu().data.view(-1).numpy())\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train_auc : 0.8327374484550133  , Valid_auc :  0.7801049393201593  ,& Test_auc :  0.7790686982792592  Avg Loss:  0.6071783783435821 Train Time (1m 20s) Eval Time (1m 55s)\n",
      "Epoch  1  Train_auc : 0.8854537905792891  , Valid_auc :  0.8154181529924693  ,& Test_auc :  0.8116567525473344  Avg Loss:  0.5004394273757935 Train Time (1m 21s) Eval Time (1m 58s)\n",
      "Epoch  2  Train_auc : 0.9286089853066386  , Valid_auc :  0.8353050349483168  ,& Test_auc :  0.8319824172937269  Avg Loss:  0.4235741053819656 Train Time (1m 19s) Eval Time (1m 55s)\n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "batch_size=100\n",
    "## Optim param\n",
    "learning_rate = 0.01\n",
    "l2=1e-04\n",
    "epsl=1e-06 \n",
    "## for final results reporting\n",
    "current_loss_l=[]\n",
    "all_losses_l=[]\n",
    "train_auc_allep =[]\n",
    "valid_auc_allep =[]\n",
    "test_auc_allep=[]\n",
    "bestValidAuc = 0.0\n",
    "bestTestAuc = 0.0\n",
    "bestValidEpoch = 0\n",
    "\n",
    "### Define Model Parameters\n",
    "model = EHR_RNN(input_size=15816, hidden_size=64 ,embed_dim=64, dropout_r=0, cell_type='GRU', n_layers=1,bi=False ,time=False, preTrainEmb='')\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "    \n",
    "### Select Optimizer\n",
    "####################\n",
    "#optimizer = optim.SGD(tmodel.parameters(), lr=learning_rate)#, weight_decay=l2)\n",
    "#optimizer = optim.Adadelta(tmodel.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "#optimizer = optim.ASGD(tmodel.parameters(), lr=learning_rate, weight_decay=l2 )\n",
    "#optimizer = optim.SparseAdam (tmodel.parameters(),lr=learning_rate) #'''lr=learning_rate,''' \n",
    "#optimizer = optim.Adagrad (tmodel.parameters(),lr=learning_rate, weight_decay=l2) #'''lr=learning_rate,''' \n",
    "optimizer = optim.Adamax(model.parameters(), lr=learning_rate, weight_decay=l2 ,eps=epsl)\n",
    "#optimizer = optim.Adamax(filter(lambda p: p.requires_grad, tmodel.parameters()), lr=learning_rate, weight_decay=l2 ,eps=epsl) ### Beta defaults (0.9, 0.999)\n",
    "#optimizer = optim.RMSprop (tmodel.parameters(),lr=learning_rate, weight_decay=l2 ,eps=epsl)\n",
    "#optimizer = optim.Adam(tmodel.parameters(), lr=learning_rate, weight_decay=learning_rate)\n",
    "    \n",
    "### Run Epochs    \n",
    "for ep in range(epochs):\n",
    "    \n",
    "    #print (model.embed.weight.data[135] ) ## checkpoint  for embedding\n",
    "    #print (model.state_dict() )## checkpoint  for all learnable parameters\n",
    "    start = time.time()\n",
    "    current_loss_la,all_losses_la = run_model_train(model,train_sl,batch_size,optimizer)\n",
    "    train_time = timeSince(start)\n",
    "    eval_start = time.time()\n",
    "    train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "    test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "    valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "    eval_time = timeSince(eval_start)\n",
    "    all_losses_l.append (all_losses_la)\n",
    "    avg_loss = np.mean(all_losses_la)\n",
    "    train_auc_allep.append(train_auc)\n",
    "    valid_auc_allep.append(valid_auc)\n",
    "    test_auc_allep.append(test_auc)\n",
    "    current_loss_l.append(current_loss_la)\n",
    "    print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "     \n",
    "    if valid_auc > bestValidAuc: \n",
    "        bestValidAuc = valid_auc\n",
    "        bestValidEpoch = ep\n",
    "        bestTestAuc = test_auc\n",
    "        best_model = model\n",
    "        #torch.save(best_model, bmodel_pth)\n",
    "        #torch.save(best_model.state_dict(), bmodel_st)\n",
    "    if ep - bestValidEpoch >12: break\n",
    "            \n",
    "print ('bestValidAuc %f has a TestAuc of %f at epoch %d ' % (bestValidAuc, bestTestAuc, bestValidEpoch))\n",
    "\n",
    "\n",
    "# for results comparison\n",
    "# Baseline GRU Epoch  6  Train_auc : 0.8908693216988484  , Valid_auc :  0.8325830431077114  ,& Test_auc :  0.8366408413283413  Avg Loss:  0.2512624728982732 Train Time (1m 0s) Eval Time (1m 24s)\n",
    "# Same as above using pack padded seq Epoch  6  Train_auc : 0.8872790431179439  , Valid_auc :  0.8216659126061384  ,& Test_auc :  0.8356468887718888  Avg Loss:  0.24958992269584687 Train Time (0m 54s) Eval Time (1m 18s)\n",
    "# Baseline GRU with time Epoch  3  Train_auc : 0.8761884834388459  , Valid_auc :  0.8256302543112971  ,& Test_auc :  0.8427002895752896  Avg Loss:  0.2645566132109044 Train Time (0m 56s) Eval Time (1m 20s)\n",
    "## Bi-directional GRU with time using log time Epoch  4  Train_auc : 0.8847842143005156  , Valid_auc :  0.841847991919277  ,& Test_auc :  0.8389933358683359  Avg Loss:  0.256076513887462 Train Time (2m 8s) Eval Time (3m 5s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
