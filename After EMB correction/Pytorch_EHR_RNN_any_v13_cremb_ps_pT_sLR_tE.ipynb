{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is playing with pytorch framework for EHR modeling. In general, a patient's health record can be represented as a sequence of visits. Each visit has certain features, and can be represented as a list of medical codes.\n",
    "\n",
    "For simplicity, we are starting with the data structure that a patient's health record is a list of list, following the line of work from Jimeng Sun's lab. We will use codes from Ed Choi to manipulate the data. \n",
    "\n",
    "The core model is an RNN , either LSTM, GRU or Vanilla RNN.\n",
    "\n",
    "# todos:\n",
    "* None for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import sklearn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "random.seed(9001)\n",
    "# for windows only    \n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "#%env CUDA_VISIBLE_DEVICES=1\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29912 4272 8545\n"
     ]
    }
   ],
   "source": [
    "train_s = pickle.load(open('pdata_3hosp/h143_train', 'rb'), encoding='bytes')\n",
    "valid_sl = pickle.load(open('pdata_3hosp/h143_valid', 'rb'), encoding='bytes')\n",
    "test_sl =  pickle.load(open('pdata_3hosp/h143_test', 'rb'), encoding='bytes')\n",
    "train_size=len(train_s)\n",
    "#test_sh_143 = pickle.load(open('pdata_3hosp/h143_test_sh', 'rb'), encoding='bytes')\n",
    "#test_l_143 = pickle.load(open('pdata_3hosp/h143_test_lng', 'rb'), encoding='bytes')\n",
    "print (train_size,len(valid_sl),len(test_sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def load_params(model):\n",
    "    params = OrderedDict()\n",
    "    weights = np.load(model)\n",
    "    for k,v in weights.iteritems():\n",
    "        params[k] = v\n",
    "    return params\n",
    "\n",
    "param= load_params(\"/data/projects/cerner_dev/cl2_hosp_retain_out/h50_cl2_all_hosp_143_cl2m.out.0.npz\")\n",
    "emb_pretrain = param['W_emb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_RNN(nn.Module):\n",
    "    def __init__(self, input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='LSTM',bi=False , preTrainEmb=''):\n",
    "        super(EHR_RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cell_type = cell_type\n",
    "        self.preTrainEmb=preTrainEmb\n",
    "        if bi: self.bi=2 \n",
    "        else: self.bi=1\n",
    "              \n",
    "        if len(self.preTrainEmb)>0:\n",
    "            emb_t= torch.FloatTensor(np.asmatrix(self.preTrainEmb))\n",
    "            self.embed= nn.Embedding.from_pretrained(emb_t)#,freeze=False)  \n",
    "        else:\n",
    "            self.embed= nn.Embedding(input_size, self.embed_dim,padding_idx=0)\n",
    "        \n",
    "        if self.cell_type == \"GRU\":\n",
    "            cell = nn.GRU\n",
    "        elif self.cell_type == \"RNN\":\n",
    "            cell = nn.RNN\n",
    "        elif self.cell_type == \"LSTM\":\n",
    "            cell = nn.LSTM\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "      \n",
    "        self.rnn_c = cell(self.embed_dim, hidden_size,num_layers=n_layers, dropout= dropout_r , bidirectional=bi , batch_first=True )\n",
    "        self.out = nn.Linear(self.hidden_size*self.bi,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def EmbedPatient_MB(self, input): # x is a ehr_seq_tensor\n",
    "        \n",
    "        mb=[]\n",
    "        lbt=[]\n",
    "        seq_l=[]\n",
    "        lp= len(max(input, key=lambda xmb: len(xmb[1]))[1])\n",
    "        #print (lp)\n",
    "        llv=0\n",
    "        for x in input:\n",
    "            lv= len(max(x[1], key=lambda xmb: len(xmb)))\n",
    "            if llv< lv:\n",
    "                llv=lv\n",
    "        #print (llv)\n",
    "        for pt in input:\n",
    "            label, ehr_seq_l = pt\n",
    "            lpx=len(ehr_seq_l)\n",
    "            seq_l.append(lpx)\n",
    "            label_tensor = Variable(torch.FloatTensor([[float(label)]]))\n",
    "            if use_cuda:\n",
    "                label_tensor = label_tensor.cuda()\n",
    "            lbt.append(label_tensor)\n",
    "            ml=(len(max(ehr_seq_l, key=len)))\n",
    "            ehr_seq_tl=[]\n",
    "            for ehr_seq in ehr_seq_l: \n",
    "                pd=(0, llv-len(ehr_seq))\n",
    "                result = F.pad(torch.from_numpy(np.asarray(ehr_seq,dtype=int)).type(torch.cuda.LongTensor),pd,\"constant\", 0)\n",
    "                if use_cuda:\n",
    "                    result.cuda()\n",
    "                ehr_seq_tl.append(result)\n",
    "            ehr_seq_t= Variable(torch.stack(ehr_seq_tl,0))     \n",
    "            lpp= lp-lpx\n",
    "            zp= nn.ZeroPad2d((0,0,0,lpp))\n",
    "            ehr_seq_t= zp(ehr_seq_t)\n",
    "            mb.append(ehr_seq_t)\n",
    "                \n",
    "        mb_t= Variable(torch.stack(mb,0)) \n",
    "        if use_cuda:\n",
    "            mb_t.cuda()\n",
    "        embedded = self.embed(mb_t)\n",
    "        embedded = torch.sum(embedded, dim=2) \n",
    "        lbt_t= Variable(torch.stack(lbt,0)) \n",
    "        return embedded, lbt_t,seq_l\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x_in , lt ,x_lens = self.EmbedPatient_MB(input)\n",
    "        x_inp = nn.utils.rnn.pack_padded_sequence(x_in,x_lens,batch_first=True)\n",
    "        output, hidden = self.rnn_c(x_inp) \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden=hidden[0]\n",
    "        if self.bi==2:\n",
    "            output = self.sigmoid(self.out(torch.cat((hidden[-2],hidden[-1]),1)))\n",
    "        else:\n",
    "            output = self.sigmoid(self.out(hidden[-1]))\n",
    "        return output.squeeze(), lt.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_pretrain=''                       \n",
    "model = EHR_RNN(input_size=15816, hidden_size=128 ,embed_dim=128, dropout_r=0, cell_type='GRU',bi=False , n_layers=1, preTrainEmb=emb_pretrain)\n",
    "#bmodel_pth = './best_model_GRUPre.pth'\n",
    "#bmodel_st = './best_model_GRUPre.st'\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (mini_batch, criterion, optimizer):  \n",
    "    \n",
    "    model.zero_grad()\n",
    "    output , label_tensor = model(mini_batch,)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train(dataset,batch_size,optimizer):#learning_rate = 0.01, l2=1e-05,epsl=1e-06 ):\n",
    "    \n",
    "\n",
    "    dataset.sort(key=lambda pt:len(pt[1]),reverse=True) \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    #print('number of Batches',n_batches)\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "            # Print iter number, loss, name and guess\n",
    "            #if iter % print_every == 0:\n",
    "               #print('%d %d%% (%s) %.4f ' % ( iter, iter/ n_batches * 100, timeSince(start), loss))\n",
    "\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "\n",
    "    dataset.sort(key=lambda pt:len(pt[1]),reverse=True) \n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "    \n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = test_model(batch)\n",
    "            y_hat.extend(output.cpu().data.view(-1).numpy())\n",
    "            labelVec.extend(label_t.cpu().data.view(-1).numpy())\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgindybekhet/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning:\n",
      "\n",
      "Mean of empty slice.\n",
      "\n",
      "/home/lgindybekhet/.local/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train_auc : 0.7288954635108481  , Valid_auc :  0.5836979334862222  ,& Test_auc :  0.582973977048154  Avg Loss:  nan ,Lr  0.01  Train Time (0m 0s) Eval Time (0m 16s)\n",
      "Epoch  1  Train_auc : 0.8813609467455621  , Valid_auc :  0.6030423690239862  ,& Test_auc :  0.6029074069591972  Avg Loss:  nan ,Lr  0.01  Train Time (0m 0s) Eval Time (0m 16s)\n",
      "Epoch  2  Train_auc : 0.9565088757396449  , Valid_auc :  0.6155587138753513  ,& Test_auc :  0.6121844284351827  Avg Loss:  nan ,Lr  0.01  Train Time (0m 0s) Eval Time (0m 16s)\n",
      "Epoch  3  Train_auc : 0.9885601577909271  , Valid_auc :  0.6359987257436395  ,& Test_auc :  0.6304554417695967  Avg Loss:  nan ,Lr  0.01  Train Time (0m 0s) Eval Time (0m 16s)\n",
      "Epoch  4  Train_auc : 0.995266272189349  , Valid_auc :  0.6465446724663908  ,& Test_auc :  0.6404866423345952  Avg Loss:  nan ,Lr  0.01  Train Time (0m 0s) Eval Time (0m 17s)\n"
     ]
    }
   ],
   "source": [
    "te_results=[]\n",
    "te_t_s=[]\n",
    "te_d_s=[]\n",
    "for percent_t in [0.01,0.02,0.04,0.06,0.1,0.2,0.4,0.6,1]:#,2,4,6,10,15,25,50,75,100]\n",
    "    emb_pretrain=''                       \n",
    "    model = EHR_RNN(input_size=15816, hidden_size=128 ,embed_dim=128, dropout_r=0, cell_type='GRU',bi=False , n_layers=1, preTrainEmb=emb_pretrain)\n",
    "    #bmodel_pth = './best_model_GRUPre.pth'\n",
    "    #bmodel_st = './best_model_GRUPre.st'\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    nTrain = int(percent_t * train_size)\n",
    "    te_res=[percent_t,nTrain]\n",
    "    random.shuffle(train_s)\n",
    "    train_sl = train_s[:nTrain]\n",
    "    epochs=100\n",
    "    batch_size=100\n",
    "    current_loss_l=[]\n",
    "    all_losses_l=[]\n",
    "    train_auc_allep =[]\n",
    "    valid_auc_allep =[]\n",
    "    test_auc_allep=[]\n",
    "    #tests_auc_allep=[]\n",
    "    #testl_auc_allep=[]\n",
    "    bestValidAuc = 0.0\n",
    "    bestTestAuc = 0.0\n",
    "    bestValidEpoch = 0\n",
    "    learning_rate= 0.01\n",
    "    l2= 1e-05\n",
    "    epsl=1e-06\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)#, weight_decay=l2)##lr 0.1\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=l2)## slow performing Lr 1 is the default and make it converge faster\n",
    "    #optimizer = optim.ASGD(model.parameters(), lr=learning_rate, weight_decay=l2 )##lr 0.1\n",
    "    #optimizer = optim.SparseAdam (model.parameters(),lr=learning_rate) #'''lr=learning_rate,''' \n",
    "    optimizer = optim.Adamax (model.parameters(),lr=learning_rate, weight_decay=l2,eps=epsl) #'''lr=learning_rate,''' \n",
    "    #optimizer = optim.Adamax(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=l2 ,eps=epsl) ### Beta defaults (0.9, 0.999)\n",
    "    #optimizer = optim.RMSprop (model.parameters(),lr=learning_rate, weight_decay=l2 ,eps=epsl)#lr 0.001\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2,eps=epsl )\n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=l2 )\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max',patience=3, verbose= True, threshold=0.0001, threshold_mode='abs')\n",
    "    for ep in range(epochs):\n",
    "        start = time.time()\n",
    "        current_loss_la,all_losses_la = run_model_train(train_sl,batch_size,optimizer)\n",
    "        train_time = timeSince(start)\n",
    "        eval_start = time.time()\n",
    "        train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "        test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "        valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "        scheduler.step(valid_auc)\n",
    "        eval_time = timeSince(eval_start)\n",
    "        all_losses_l.append (all_losses_la)\n",
    "        avg_loss = np.mean(all_losses_la)\n",
    "        train_auc_allep.append(train_auc)\n",
    "        valid_auc_allep.append(valid_auc)\n",
    "        test_auc_allep.append(test_auc)\n",
    "        #testl_auc_allep.append(test_l_auc)\n",
    "        #tests_auc_allep.append(test_sh_auc)\n",
    "        current_loss_l.append(current_loss_la)\n",
    "        print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\" Avg Loss: \", avg_loss, ',Lr ',optimizer.param_groups[0]['lr'], ' Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "        #print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\"less than 5 visits history :\",test_sh_auc,\" for more :\",test_l_auc ,\" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "        if valid_auc > bestValidAuc: \n",
    "            bestValidAuc = valid_auc\n",
    "            bestValidEpoch = ep\n",
    "            bestTestAuc = test_auc\n",
    "            train_err= round(1-train_auc,3)\n",
    "            dev_err=round(1-valid_auc,3)\n",
    "            #best_model = model\n",
    "            #torch.save(best_model, bmodel_pth)\n",
    "            #torch.save(best_model.state_dict(), bmodel_st)\n",
    "        if ep - bestValidEpoch >6: break\n",
    "    te_t_s.append(train_err)\n",
    "    te_d_s.append(dev_err)\n",
    "    te_res.append(train_auc_allep)\n",
    "    te_res.append(valid_auc_allep)\n",
    "    te_res.append(test_auc_allep)\n",
    "    print ('bestValidAuc %f has a TestAuc of %f at epoch %d ' % (bestValidAuc, bestTestAuc, bestValidEpoch))\n",
    "    te_results.append(te_res)\n",
    "    \n",
    "    #torch.save(best_model, bmodel_pth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot trainiing versus dev error\n",
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('LailaRasmy','mzNHzVvwYjcZwBDZx3B7')\n",
    "\n",
    "x_l=[0.01,0.02,0.04,0.06,0.1,0.2,0.4,0.6,1]\n",
    "train_err_fg= go.Scatter(x=x_l, y=te_t_s, name='train_err')\n",
    "dev_err_fg= go.Scatter(x=x_l, y=te_d_s, name='dev_err')\n",
    "\n",
    "train_err_max = max(te_t_s)\n",
    "dev_err_min = min(te_d_s)\n",
    "test_max = max(test_auc_allep)\n",
    "data = [train_err_fg,dev_err_fg]\n",
    "layout = go.Layout(xaxis=dict(dtick=0.1))\n",
    "#layout.update(dict(annotations=[go.Annotation(text=\"Max train error \", x=x_l[te_t_s.index(train_err_max)], y=train_err_max)]))\n",
    "layout.update(dict(annotations=[go.Annotation(text=\"Min Dev error \", x=x_l[te_d_s.index(dev_err_min)], y=dev_err_min)]))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Training Dev error plot')\n",
    "#url = py.plot(data, filename='some-data')  # gen. online plot\n",
    "#py.image.save_as(data, 'some-data.png') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
