{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import sklearn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "# for windows only    \n",
    "#import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# H143 no encounter data input_size= 15816\n",
    "in_size=15816\n",
    "train_sl = pickle.load(open('pdata_3hosp/h143_train', 'rb'), encoding='bytes')\n",
    "valid_sl = pickle.load(open('pdata_3hosp/h143_valid', 'rb'), encoding='bytes')\n",
    "test_sl = pickle.load(open('pdata_3hosp/h143_test', 'rb'), encoding='bytes')\n",
    "\n",
    "# H143 with encounter data input_size= 18250\n",
    "in_size= 18250\n",
    "train_sl = pickle.load(open('/data/projects/py_ehr_2/Data/hf50_h143_cl2withEnc_c1_143.train', 'rb'), encoding='bytes')\n",
    "valid_sl = pickle.load(open('/data/projects/py_ehr_2/Data/hf50_h143_cl2withEnc_c1_143.valid', 'rb'), encoding='bytes')\n",
    "test_sl = pickle.load(open('/data/projects/py_ehr_2/Data/hf50_h143_cl2withEnc_c1_143.test', 'rb'), encoding='bytes')\n",
    "\n",
    "# Readmission no encounter input_size= 8690\n",
    "in_size= 8690\n",
    "train_sl = pickle.load(open('/data/projects/Readm/pdata/readm_diag_comb_h143.train', 'rb'), encoding='bytes')\n",
    "valid_sl = pickle.load(open('/data/projects/Readm/pdata/readm_diag_comb_h143.valid', 'rb'), encoding='bytes')\n",
    "test_sl =  pickle.load(open('/data/projects/Readm/pdata/readm_diag_comb_h143.test', 'rb'), encoding='bytes')\n",
    "\n",
    "'''\n",
    "# Readmission with encounter input_size= 12433\n",
    "in_size= 12433\n",
    "train_sl = pickle.load(open('/data/projects/Readm/pdata/readm_diag_enc_comb_h143.train', 'rb'), encoding='bytes')\n",
    "valid_sl = pickle.load(open('/data/projects/Readm/pdata/readm_diag_enc_comb_h143.valid', 'rb'), encoding='bytes')\n",
    "test_sl =  pickle.load(open('/data/projects/Readm/pdata/readm_diag_enc_comb_h143.test', 'rb'), encoding='bytes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_RNN(nn.Module):\n",
    "    def __init__(self, input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='LSTM',bi=False , preTrainEmb=''):\n",
    "        super(EHR_RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cell_type = cell_type\n",
    "        self.preTrainEmb=preTrainEmb\n",
    "        if bi: self.bi=2 \n",
    "        else: self.bi=1\n",
    "              \n",
    "        if len(self.preTrainEmb)>0:\n",
    "            emb_t= torch.FloatTensor(np.asmatrix(self.preTrainEmb))\n",
    "            self.embed= nn.Embedding.from_pretrained(emb_t)#,freeze=False)  \n",
    "        else:\n",
    "            self.embed= nn.Embedding(input_size, self.embed_dim,padding_idx=0)\n",
    "        \n",
    "        if self.cell_type == \"GRU\":\n",
    "            cell = nn.GRU\n",
    "        elif self.cell_type == \"RNN\":\n",
    "            cell = nn.RNN\n",
    "        elif self.cell_type == \"LSTM\":\n",
    "            cell = nn.LSTM\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "      \n",
    "        self.rnn_c = cell(self.embed_dim, hidden_size,num_layers=n_layers, dropout= dropout_r , bidirectional=bi , batch_first=True )\n",
    "        self.out = nn.Linear(self.hidden_size*self.bi,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def EmbedPatient_MB(self, input): # x is a ehr_seq_tensor\n",
    "        \n",
    "        mb=[]\n",
    "        lbt=[]\n",
    "        seq_l=[]\n",
    "        self.bsize=len(input)\n",
    "        lp= len(max(input, key=lambda xmb: len(xmb[1]))[1])\n",
    "        #print (lp)\n",
    "        llv=0\n",
    "        for x in input:\n",
    "            lv= len(max(x[1], key=lambda xmb: len(xmb)))\n",
    "            if llv< lv:\n",
    "                llv=lv\n",
    "        #print (llv)\n",
    "        for pt in input:\n",
    "            label, ehr_seq_l = pt\n",
    "            lpx=len(ehr_seq_l)\n",
    "            seq_l.append(lpx)\n",
    "            label_tensor = Variable(torch.FloatTensor([[float(label)]]))\n",
    "            if use_cuda:\n",
    "                label_tensor = label_tensor.cuda()\n",
    "            lbt.append(label_tensor)\n",
    "            ml=(len(max(ehr_seq_l, key=len)))\n",
    "            ehr_seq_tl=[]\n",
    "            for ehr_seq in ehr_seq_l: \n",
    "                pd=(0, llv-len(ehr_seq))\n",
    "                result = F.pad(torch.from_numpy(np.asarray(ehr_seq,dtype=int)).type(torch.cuda.LongTensor),pd,\"constant\", 0)\n",
    "                if use_cuda:\n",
    "                    result.cuda()\n",
    "                ehr_seq_tl.append(result)\n",
    "            ehr_seq_t= Variable(torch.stack(ehr_seq_tl,0))     \n",
    "            lpp= lp-lpx\n",
    "            zp= nn.ZeroPad2d((0,0,0,lpp))\n",
    "            ehr_seq_t= zp(ehr_seq_t)\n",
    "            mb.append(ehr_seq_t)\n",
    "                \n",
    "        mb_t= Variable(torch.stack(mb,0)) \n",
    "        if use_cuda:\n",
    "            mb_t.cuda()\n",
    "        embedded = self.embed(mb_t)\n",
    "        embedded = torch.sum(embedded, dim=2) \n",
    "        lbt_t= Variable(torch.stack(lbt,0)) \n",
    "        return embedded, lbt_t,seq_l\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        h_0 = Variable(torch.rand(self.n_layers*self.bi,self.bsize, self.hidden_size))\n",
    "\n",
    "        if use_cuda:\n",
    "            h_0 =h_0.cuda()\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            result = (h_0,h_0)\n",
    "        else: \n",
    "            result = h_0\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x_in , lt ,x_lens = self.EmbedPatient_MB(input)\n",
    "        x_inp = nn.utils.rnn.pack_padded_sequence(x_in,x_lens,batch_first=True)\n",
    "        h_0 = self.init_hidden()\n",
    "        output, hidden = self.rnn_c(x_inp,h_0) \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden=hidden[0]\n",
    "        if self.bi==2:\n",
    "            output = self.sigmoid(self.out(torch.cat((hidden[-2],hidden[-1]),1)))\n",
    "        else:\n",
    "            output = self.sigmoid(self.out(hidden[-1]))\n",
    "        return output.squeeze(), lt.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (tmodel,mini_batch, criterion, optimizer):  \n",
    "    \n",
    "    tmodel.train()\n",
    "    tmodel.zero_grad()\n",
    "    output , label_tensor = tmodel(mini_batch)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train(tmodel,dataset,batch_size,learning_rate = 0.01, l2=1e-04,epsl=1e-06 ):\n",
    "    \n",
    "    #optimizer = optim.SGD(tmodel.parameters(), lr=learning_rate)#, weight_decay=l2)\n",
    "    #optimizer = optim.Adadelta(tmodel.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    #optimizer = optim.ASGD(tmodel.parameters(), lr=learning_rate, weight_decay=l2 )\n",
    "    #optimizer = optim.SparseAdam (tmodel.parameters(),lr=learning_rate) #'''lr=learning_rate,''' \n",
    "    optimizer = optim.Adagrad (tmodel.parameters(),lr=learning_rate, weight_decay=l2) #'''lr=learning_rate,''' \n",
    "    #optimizer = optim.Adamax(tmodel.parameters(), lr=learning_rate, weight_decay=l2 ,eps=epsl)\n",
    "    #optimizer = optim.Adamax(filter(lambda p: p.requires_grad, tmodel.parameters()), lr=learning_rate, weight_decay=l2 ,eps=epsl) ### Beta defaults (0.9, 0.999)\n",
    "    #optimizer = optim.RMSprop (tmodel.parameters(),lr=learning_rate, weight_decay=l2 ,eps=epsl)\n",
    "    #optimizer = optim.Adam(tmodel.parameters(), lr=learning_rate, weight_decay=learning_rate)\n",
    "    dataset.sort(key=lambda pt:len(pt[1]),reverse=True) \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(tmodel,batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "    test_model.eval()\n",
    "    dataset.sort(key=lambda pt:len(pt[1]),reverse=True) \n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "    \n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = test_model(batch)\n",
    "            y_hat.extend(output.cpu().data.view(-1).numpy())\n",
    "            labelVec.extend(label_t.cpu().data.view(-1).numpy())\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train_auc : 0.7115027026599061  , Valid_auc :  0.6450256651870956  ,& Test_auc :  0.6671947338950875  Avg Loss:  0.6798904563251295 Train Time (0m 38s) Eval Time (0m 53s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgindybekhet/.local/lib/python3.5/site-packages/torch/serialization.py:193: UserWarning:\n",
      "\n",
      "Couldn't retrieve source code for container of type EHR_RNN. It won't be checked for correctness upon loading.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train_auc : 0.7402763978799356  , Valid_auc :  0.6648200957175093  ,& Test_auc :  0.6896989954721131  Avg Loss:  0.6466100517072175 Train Time (0m 46s) Eval Time (1m 4s)\n",
      "Epoch  2  Train_auc : 0.7659130698870945  , Valid_auc :  0.6749225084930692  ,& Test_auc :  0.6952243023491127  Avg Loss:  0.62762205726222 Train Time (1m 18s) Eval Time (1m 20s)\n",
      "Epoch  3  Train_auc : 0.7909376314483805  , Valid_auc :  0.6748121605871996  ,& Test_auc :  0.7005390134495744  Avg Loss:  0.6154361982094613 Train Time (0m 37s) Eval Time (0m 52s)\n",
      "Epoch  4  Train_auc : 0.8156408676973573  , Valid_auc :  0.686641704069234  ,& Test_auc :  0.7043576720181066  Avg Loss:  0.5958315849304199 Train Time (0m 38s) Eval Time (0m 50s)\n",
      "Epoch  5  Train_auc : 0.8408102001101121  , Valid_auc :  0.6922917648225755  ,& Test_auc :  0.7049672668672163  Avg Loss:  0.5836552949328171 Train Time (0m 45s) Eval Time (1m 3s)\n",
      "Epoch  6  Train_auc : 0.8645343616217016  , Valid_auc :  0.6931770277977534  ,& Test_auc :  0.7058925225928763  Avg Loss:  0.5623949888505433 Train Time (0m 36s) Eval Time (0m 49s)\n",
      "Epoch  7  Train_auc : 0.8866862812349774  , Valid_auc :  0.704043816797679  ,& Test_auc :  0.7087803385175795  Avg Loss:  0.5451407761950241 Train Time (0m 45s) Eval Time (0m 59s)\n",
      "Epoch  8  Train_auc : 0.9014811263471684  , Valid_auc :  0.6957968606640712  ,& Test_auc :  0.7038927162727062  Avg Loss:  0.5291930236314474 Train Time (0m 36s) Eval Time (0m 50s)\n",
      "Epoch  9  Train_auc : 0.921021841645608  , Valid_auc :  0.6908795596002679  ,& Test_auc :  0.7006144368448562  Avg Loss:  0.5037030085137016 Train Time (0m 38s) Eval Time (1m 1s)\n",
      "Epoch  10  Train_auc : 0.9368154533761222  , Valid_auc :  0.6902571478165993  ,& Test_auc :  0.6962383279967919  Avg Loss:  0.48293746803936205 Train Time (0m 44s) Eval Time (0m 59s)\n",
      "Epoch  11  Train_auc : 0.9543631270138502  , Valid_auc :  0.6957708235177424  ,& Test_auc :  0.6992760595590308  Avg Loss:  0.46043572990517856 Train Time (0m 44s) Eval Time (0m 59s)\n",
      "Epoch  12  Train_auc : 0.9670426682458068  , Valid_auc :  0.6927914300592655  ,& Test_auc :  0.6918535283871313  Avg Loss:  0.4389030277729034 Train Time (0m 44s) Eval Time (1m 2s)\n",
      "Epoch  13  Train_auc : 0.9744755158866786  , Valid_auc :  0.6977409675899522  ,& Test_auc :  0.6952469604061318  Avg Loss:  0.4136952751561215 Train Time (0m 44s) Eval Time (1m 1s)\n",
      "Epoch  14  Train_auc : 0.9821753965985421  , Valid_auc :  0.687955960026781  ,& Test_auc :  0.6871462394453804  Avg Loss:  0.38927969367880566 Train Time (0m 44s) Eval Time (0m 59s)\n",
      "Epoch  15  Train_auc : 0.9873549703119517  , Valid_auc :  0.6940288144419371  ,& Test_auc :  0.6818815002240974  Avg Loss:  0.36071693756078416 Train Time (0m 42s) Eval Time (1m 2s)\n",
      "Epoch  16  Train_auc : 0.9926087945970352  , Valid_auc :  0.6834639323530141  ,& Test_auc :  0.6796172464439265  Avg Loss:  0.32398881708320815 Train Time (0m 43s) Eval Time (1m 1s)\n",
      "Epoch  17  Train_auc : 0.9954257993685605  , Valid_auc :  0.6822476256602277  ,& Test_auc :  0.674922062491542  Avg Loss:  0.29788802313177204 Train Time (0m 43s) Eval Time (1m 1s)\n",
      "Epoch  18  Train_auc : 0.9973782392129095  , Valid_auc :  0.691306072854415  ,& Test_auc :  0.6735653725294959  Avg Loss:  0.2684777212770362 Train Time (0m 43s) Eval Time (1m 2s)\n",
      "Epoch  19  Train_auc : 0.9980073643574519  , Valid_auc :  0.6835333647432242  ,& Test_auc :  0.6689782023283791  Avg Loss:  0.2510567455699569 Train Time (0m 43s) Eval Time (1m 2s)\n",
      "Epoch  20  Train_auc : 0.9987686230381203  , Valid_auc :  0.6805998462568501  ,& Test_auc :  0.6665860701990061  Avg Loss:  0.23730162876216987 Train Time (0m 43s) Eval Time (1m 0s)\n",
      "bestValidAuc 0.704044 has a TestAuc of 0.708780 at epoch 7 \n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "batch_size=128\n",
    "current_loss_l=[]\n",
    "all_losses_l=[]\n",
    "train_auc_allep =[]\n",
    "valid_auc_allep =[]\n",
    "test_auc_allep=[]\n",
    "bestValidAuc = 0.0\n",
    "bestTestAuc = 0.0\n",
    "bestValidEpoch = 0\n",
    "\n",
    "### define the new model\n",
    "\n",
    "emb_pretrain=''                       \n",
    "###HF no encounter 15816, with encounter 18250 , Readm without enc 8690 , Readm with enc 12433 (in_size)\n",
    "model = EHR_RNN(input_size=in_size, hidden_size=128 ,embed_dim=256, dropout_r=0, cell_type='GRU',bi=False , n_layers=1, preTrainEmb=emb_pretrain)\n",
    "bmodel_pth = './best_model_BiLSTM.pth'\n",
    "bmodel_st = './best_model_BiLSTM.st'\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "### Run Epochs    \n",
    "for ep in range(epochs):\n",
    "    \n",
    "    #print (model.embed.weight.data[135] )\n",
    "    start = time.time()\n",
    "    current_loss_la,all_losses_la = run_model_train(model,train_sl,batch_size)\n",
    "    train_time = timeSince(start)\n",
    "    eval_start = time.time()\n",
    "    train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "    test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "    valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "    eval_time = timeSince(eval_start)\n",
    "    all_losses_l.append (all_losses_la)\n",
    "    avg_loss = np.mean(all_losses_la)\n",
    "    train_auc_allep.append(train_auc)\n",
    "    valid_auc_allep.append(valid_auc)\n",
    "    test_auc_allep.append(test_auc)\n",
    "    current_loss_l.append(current_loss_la)\n",
    "    print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "     \n",
    "    if valid_auc > bestValidAuc: \n",
    "        bestValidAuc = valid_auc\n",
    "        bestValidEpoch = ep\n",
    "        bestTestAuc = test_auc\n",
    "        best_model = model\n",
    "        torch.save(best_model, bmodel_pth)\n",
    "        torch.save(best_model.state_dict(), bmodel_st)\n",
    "    if ep - bestValidEpoch >12: break\n",
    "            \n",
    "print ('bestValidAuc %f has a TestAuc of %f at epoch %d ' % (bestValidAuc, bestTestAuc, bestValidEpoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~LailaRasmy/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('LailaRasmy','mzNHzVvwYjcZwBDZx3B7')\n",
    "\n",
    "train_auc_fg= go.Scatter(x= np.arange(epochs), y=train_auc_allep, name='train')\n",
    "test_auc_fg= go.Scatter(x= np.arange(epochs), y=test_auc_allep, name='test')\n",
    "valid_auc_fg= go.Scatter(x= np.arange(epochs), y=valid_auc_allep, name='valid')\n",
    "valid_max = max(valid_auc_allep)\n",
    "test_max = max(test_auc_allep)\n",
    "data = [train_auc_fg,test_auc_fg,valid_auc_fg]\n",
    "layout = go.Layout(xaxis=dict(dtick=1))\n",
    "layout.update(dict(annotations=[go.Annotation(text=\"Max Valid\", x=valid_auc_allep.index(valid_max), y=valid_max)]))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='BiRNN_Auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
