{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is playing with pytorch framework for EHR modeling. In general, a patient's health record can be represented as a sequence of visits. Each visit has certain features, and can be represented as a list of medical codes.\n",
    "\n",
    "For simplicity, we are starting with the data structure that a patient's health record is a list of list, following the line of work from Jimeng Sun's lab. We will use codes from Ed Choi to manipulate the data. \n",
    "\n",
    "The core model is an GRU.\n",
    "\n",
    "# todos:\n",
    "* dropout\n",
    "* L1/L2 regularization\n",
    "* validation and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import sklearn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data: load the input file containing list of list of list, and corresponding label file\n",
    "# and output the splitted training, valid and Test sets\n",
    "\n",
    "def data_load_split_VT(seqFile = 'pytorch_ehr-master/pytorch_ehr-master/data/cerner/hospital_data/h143.visits', labelFile = 'pytorch_ehr-master/pytorch_ehr-master/data/cerner/hospital_data/h143.labels' , test_r=0.2 , valid_r=0.1):\n",
    "\n",
    "    set_x = pickle.load(open(seqFile, 'rb'), encoding='bytes')\n",
    "    set_y = pickle.load(open(labelFile, 'rb'),encoding='bytes')\n",
    "    merged_set = [[set_y[i],set_x[i]] for i in range(len(set_x))] # merge the two lists\n",
    "\n",
    "    # set random seed\n",
    "    random.seed( 3 )\n",
    "    \n",
    "    dataSize = len(merged_set)\n",
    "    nTest = int(test_r * dataSize)\n",
    "    nValid = int(valid_r * dataSize)\n",
    "    \n",
    "    random.shuffle(merged_set)\n",
    "\n",
    "    test_set = merged_set[:nTest]\n",
    "    valid_set = merged_set[nTest:nTest+nValid]\n",
    "    train_set = merged_set[nTest+nValid:]\n",
    "\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sl , valid_sl , test_sl = data_load_split_VT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8545"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    def forward(self, input, hidden):\\n        \\n        x_in , lt = self.EmbedPatient_MB(input)\\n        \\n        for i in range(self.n_layers):\\n                output, hidden = self.gru(x_in, hidden) # input (seq_len, batch, input_size) need to check torch.nn.utils.rnn.pack_padded_sequence() \\n                                                          #h_0 (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\\n\\n        output = self.sigmoid(self.out(output[0]))\\n        return output, lt\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EHR_RETAIN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,embed_dim, n_layers=1,dropout_r=0.1):\n",
    "        super(EHR_RETAIN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cuda_flag = torch.cuda.is_available()\n",
    "        self.release = False # if set to true, then we return all values in computing\n",
    "\n",
    "#        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.embedBag = nn.EmbeddingBag(input_size, self.embed_dim,mode= 'sum')\n",
    "#        self.gru = nn.GRU(self.embed_dim, hidden_size, dropout= dropout_r )\n",
    "#        self.out = nn.Linear(self.hidden_size,1)\n",
    "#        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.RNN1 = nn.GRU(embed_dim,hidden_size,1,batch_first=True,bidirectional=True,dropout= dropout_r) # for alpha\n",
    "        self.RNN2 = nn.GRU(embed_dim,hidden_size,1,batch_first=True,bidirectional=True,dropout= dropout_r) # for Beta\n",
    "        self.RNN3 = nn.GRU(embed_dim,hidden_size,1,batch_first=True,bidirectional=True,dropout= dropout_r) # for attention\n",
    "        self.wa = nn.Linear(hidden_size*2,1,bias=False)\n",
    "        self.Wb = nn.Linear(hidden_size*2,hidden_size,bias=False)\n",
    "        self.W_out = nn.Linear(hidden_size,1,bias=False)###replaced num_classes with 1\n",
    "        self.wa2 = nn.Linear(hidden_size*2, 1)\n",
    "        self.w_lever = nn.Linear(hidden_size*2, 2)\n",
    "\n",
    "\n",
    "    ##def __init__(self, input_size, hidden_size, num_classes, cuda_flag=False):\n",
    "        ##super(RETAIN,self).__init__()\n",
    "        ##emb = nn.Embedding(1400,input_size)\n",
    "        ##self.emb = emb.weight\n",
    "        ##emb_trg = nn.Embedding(num_classes,num_classes)\n",
    "        ##emb_trg.weight.data.copy_(torch.from_numpy(np.eye(num_classes)))\n",
    "        ##self.emb_trg = emb_trg\n",
    "\n",
    "\n",
    "      \n",
    "    def EmbedPatient_MB(self, seq_mini_batch): # x is a ehr_seq_tensor\n",
    "        \n",
    "        lp= len(max(seq_mini_batch, key=lambda xmb: len(xmb[1]))[1]) # max number of visitgs within mb ??? verify again\n",
    "        tb= torch.FloatTensor(len(seq_mini_batch),lp,self.embed_dim) \n",
    "        lbt1= torch.FloatTensor(len(seq_mini_batch),1)\n",
    "\n",
    "        for pt in range(len(seq_mini_batch)):\n",
    "            \n",
    "            lbt ,pt_visits =seq_mini_batch[pt]\n",
    "            lbt1[pt] = torch.FloatTensor([[float(lbt)]])\n",
    "            ml=(len(max(pt_visits, key=len))) ## getting the visit with max no. of codes ##the max number of visits for pts within the minibatch\n",
    "            txs= torch.LongTensor(len(pt_visits),ml)\n",
    "            \n",
    "            b=0\n",
    "            for i in pt_visits:\n",
    "                pd=(0, ml-len(i))\n",
    "                txs[b] = F.pad(torch.from_numpy(np.asarray(i)).view(1,-1),pd,\"constant\", 0).data\n",
    "                b=b+1\n",
    "            \n",
    "            if use_cuda:\n",
    "                txs=txs.cuda()\n",
    "                \n",
    "            emb_bp= self.embedBag(Variable(txs)) ### embed will be num_of_visits*max_num_codes*embed_dim \n",
    "            #### the embed Bag dim will be num_of_visits*embed_dim\n",
    "            \n",
    "            zp= nn.ZeroPad2d((0,0,0,(lp-len(pt_visits))))\n",
    "            xzp= zp(emb_bp)\n",
    "            tb[pt]=xzp.data\n",
    "\n",
    "        #tb= tb.permute(1, 0, 2) ### as my final input need to be seq_len x batch_size x input_size\n",
    "        #commented the above as they use batch first\n",
    "        emb_m=Variable(tb)\n",
    "        label_tensor = Variable(lbt1)\n",
    "\n",
    "        if use_cuda:\n",
    "                label_tensor = label_tensor.cuda()\n",
    "                emb_m = emb_m.cuda()\n",
    "                \n",
    "        return emb_m , label_tensor\n",
    "    \n",
    "\n",
    " \n",
    "    def forward(self, inputs):#,hidden):\n",
    "        # get embedding using self.emb\n",
    "        #b,seq,features = inputs.size()\n",
    "        #embedded = torch.mm(inputs.view(-1,features),self.emb).view(b,seq,-1)\n",
    "        #if self.release:\n",
    "        #    self.embedded = embedded\n",
    "            x_in , lt = self.EmbedPatient_MB(inputs)\n",
    "          #  print (lt,x_in)\n",
    "            b,seq,features = x_in.size()\n",
    "          #  print (\"b,seq,features\",b,seq,features)\n",
    "            # get alpha coefficients\n",
    "            outputs1 = self.RNN1(x_in) # [b x seq x 128*2] , the hidden on 2*b*128\n",
    "           # print('outputs1', outputs1)\n",
    "            E = self.wa(outputs1[0].contiguous().view(-1, self.hidden_size*2)) # [b*seq x 1]\n",
    "            #print('E', E)\n",
    "            alpha = F.softmax(E.view(b,seq),1) # [b x seq]\n",
    "            if self.release:\n",
    "                self.alpha = alpha\n",
    "            #print('alpha', alpha)\n",
    "            # get beta coefficients\n",
    "            outputs2 = self.RNN2(x_in) # [b x seq x 128] actually is the same as RNN1 [b x seq x 128*2] , the hidden on 2*b*128\n",
    "            #print('outputs2', outputs2)\n",
    "            outputs2 = self.Wb(outputs2[0].contiguous().view(-1,self.hidden_size*2)) # [b*seq x hid]\n",
    "            #print('outputs2 after wb', outputs2)\n",
    "\n",
    "            Beta = torch.tanh(outputs2).view(b, seq, self.hidden_size) # [b x seq x 128]\n",
    "            #print('Beta', Beta)\n",
    "\n",
    "            if self.release:\n",
    "                self.Beta = Beta\n",
    "            \n",
    "            outputs_calc = self.compute(x_in, Beta, alpha) # [b, num_classes]\n",
    "            #print('outputs_calc', outputs_calc)\n",
    "            outputs_calc = F.softmax(outputs_calc,1)\n",
    "            #print('outputs_calc after softmax', outputs_calc)\n",
    "            \n",
    "            # get outputs obtained by copying mechanism\n",
    "            outputs3 = self.RNN3(x_in) # [b, seq, 128]\n",
    "            #print('outputs3', outputs3)\n",
    "            E = self.wa2(outputs3[0].contiguous().view(-1, self.hidden_size*2)) # [b*seq x 1]\n",
    "            #print('E after out3', E)\n",
    "            alpha2 = F.softmax(E.view(b,seq),1).unsqueeze(2) # [b, seq, 1]\n",
    "            #print('alpha2', alpha2)\n",
    "            #alpha2 = alpha2[:,1:,:] # [b,seq-1,1]\n",
    "            #print('alpha2 after slicing', alpha2)\n",
    "            if self.release:\n",
    "                self.alpha2 = alpha2\n",
    "\n",
    "            # targets: list of list\n",
    "            targets = lt\n",
    "            #targets = Variable(torch.LongTensor(targets)[:,:-1],requires_grad=False)\n",
    "            if self.cuda_flag:\n",
    "                targets = targets.cuda()\n",
    "            #inputs2 = targets #self.emb_trg(targets) # [b, seq-1, num_classes]\n",
    "            #print('targets inputs2', inputs2)\n",
    "            #outputs_copy = (inputs2*alpha2).sum(1) # [b, num_classes] .expand_as(inputs2)\n",
    "            #outputs_copy = F.softmax(outputs_copy,1)\n",
    "            #print('output_copy',outputs_copy)\n",
    "\n",
    "            # for inputs, we apply another contribution score that helps add up to output\n",
    "            # this way, we can obtain a vector that takes in all of the inputs\n",
    "\n",
    "            lever = outputs3[0].sum(1) # [b, 128]\n",
    "            #print('lever [b, 128]',lever)\n",
    "            lever = F.softmax(self.w_lever(lever),1) # [b,2]\n",
    "            #print('lever after w_lever and softmax [b, 2]',lever)\n",
    "            if self.release:\n",
    "                self.lever = lever\n",
    "                self.outputs_calc = outputs_calc\n",
    "                self.outputs_copy = outputs_copy\n",
    "\n",
    "            #outputs = outputs_calc*lever[:,0:1].expand_as(outputs_calc) + outputs_copy*lever[:,1:2].expand_as(outputs_copy) # [b, num_classes]\n",
    "            outputs = outputs_calc*lever[:,0:1].expand_as(outputs_calc)\n",
    "            \n",
    "            #print('final output',outputs)\n",
    "            return outputs , targets\n",
    "\n",
    "    # multiply to inputs\n",
    "    \n",
    "    def compute(self, embedded, Beta, alpha):\n",
    "            b,seq,_ = embedded.size()\n",
    "            outputs = (embedded*Beta)*alpha.unsqueeze(2).expand(b,seq,self.hidden_size)\n",
    "            outputs = outputs.sum(1) # [b x hidden]\n",
    "            return self.W_out(outputs) # [b x num_classes]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "'''    def forward(self, input, hidden):\n",
    "        \n",
    "        x_in , lt = self.EmbedPatient_MB(input)\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "                output, hidden = self.gru(x_in, hidden) # input (seq_len, batch, input_size) need to check torch.nn.utils.rnn.pack_padded_sequence() \n",
    "                                                          #h_0 (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
    "\n",
    "        output = self.sigmoid(self.out(output[0]))\n",
    "        return output, lt\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = EHR_RETAIN(input_size=20000, hidden_size=128 ,embed_dim=128, dropout_r=0)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train (mini_batch, criterion, optimizer):  \n",
    "    \n",
    "    #hidden = model.initHidden()\n",
    "    model.zero_grad()\n",
    "    output , label_tensor = model(mini_batch)#,hidden)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variableFromEHRSeq(ehr_seq):\n",
    "    # ehr_seq is a list of list\n",
    "    result = []\n",
    "    if use_cuda:\n",
    "        for i in range(len(ehr_seq)):\n",
    "            result.append( Variable(torch.LongTensor([int(v) for v in ehr_seq[i]])).cuda() )\n",
    "    # if use_cuda:\n",
    "    #     return result.cuda()\n",
    "    else:\n",
    "        for i in range(len(ehr_seq)):\n",
    "            result.append( Variable(torch.LongTensor([int(v) for v in ehr_seq[i]])) )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model_train(dataset,batch_size,learning_rate = 0.0001 ):\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "    #optimizer = optim.RMSprop (model.parameters())\n",
    "\n",
    "    dataset.sort(key=lambda pt:len(pt[1])) \n",
    "   \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    #print('number of Batches',n_batches)\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "             #Print iter number, loss, name and guess\n",
    "            if iter % print_every == 0:\n",
    "                   print('%d %d%% (%s) %.4f ' % ( iter, iter/ n_batches * 100, timeSince(start), loss))\n",
    "\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "\n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = model(batch)\n",
    "            y_hat.append(output.cpu().data.numpy()[0][0])\n",
    "            labelVec.append(label_t.cpu().data.numpy()[0][0])\n",
    "    #print (labelVec, y_hat)\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 6% (0m 5s) 0.8907 \n",
      "20 13% (0m 8s) 0.3418 \n",
      "30 20% (0m 12s) 0.3307 \n",
      "40 26% (0m 14s) 0.5247 \n",
      "50 33% (0m 16s) 0.4654 \n",
      "60 40% (0m 19s) 0.3925 \n",
      "70 46% (0m 32s) 0.3190 \n",
      "80 53% (0m 36s) 1.6318 \n",
      "90 60% (0m 40s) 1.2937 \n",
      "100 66% (0m 47s) 0.3815 \n",
      "110 73% (0m 54s) 0.3937 \n",
      "120 80% (1m 2s) 0.3774 \n",
      "130 86% (1m 8s) 0.8249 \n",
      "140 93% (1m 13s) 0.3453 \n",
      "150 100% (1m 16s) 0.4897 \n",
      "Epoch  0  Train_auc : 0.705882352941  , Valid_auc :  0.564705882353  ,& Test_auc :  0.724358974359\n",
      "10 6% (0m 2s) 0.4544 \n",
      "20 13% (0m 11s) 0.4663 \n",
      "30 20% (0m 15s) 1.2552 \n",
      "40 26% (0m 20s) 0.3725 \n",
      "50 33% (0m 26s) 0.3533 \n",
      "60 40% (0m 28s) 0.3747 \n",
      "70 46% (0m 34s) 0.5775 \n",
      "80 53% (0m 36s) 0.3851 \n",
      "90 60% (0m 39s) 0.4254 \n",
      "100 66% (0m 43s) 1.1858 \n",
      "110 73% (0m 47s) 0.4770 \n",
      "120 80% (0m 51s) 3.8466 \n",
      "130 86% (0m 55s) 1.7707 \n",
      "140 93% (0m 57s) 0.4735 \n",
      "150 100% (1m 9s) 5.7559 \n",
      "Epoch  1  Train_auc : 0.669615214507  , Valid_auc :  0.6  ,& Test_auc :  0.75641025641\n",
      "10 6% (0m 5s) 1.6856 \n",
      "20 13% (0m 9s) 0.3540 \n",
      "30 20% (0m 12s) 0.2998 \n",
      "40 26% (0m 14s) 0.4013 \n",
      "50 33% (0m 16s) 0.3321 \n",
      "60 40% (0m 20s) 0.3681 \n",
      "70 46% (0m 31s) 0.4731 \n",
      "80 53% (0m 34s) 1.1808 \n",
      "90 60% (0m 40s) 0.5609 \n",
      "100 66% (0m 43s) 0.8985 \n",
      "110 73% (0m 46s) 2.3521 \n",
      "120 80% (0m 50s) 2.3566 \n",
      "130 86% (0m 53s) 0.3964 \n",
      "140 93% (0m 59s) 0.6254 \n",
      "150 100% (1m 9s) 0.5103 \n",
      "Epoch  2  Train_auc : 0.694383016364  , Valid_auc :  0.6  ,& Test_auc :  0.717948717949\n",
      "10 6% (0m 7s) 0.4007 \n",
      "20 13% (0m 9s) 0.3917 \n",
      "30 20% (0m 13s) 0.6115 \n",
      "40 26% (0m 17s) 0.7463 \n",
      "50 33% (0m 22s) 0.3294 \n",
      "60 40% (0m 32s) 0.3073 \n",
      "70 46% (0m 35s) 0.3416 \n",
      "80 53% (0m 37s) 0.3598 \n",
      "90 60% (0m 45s) 0.4577 \n",
      "100 66% (0m 50s) 0.3754 \n",
      "110 73% (0m 52s) 0.4290 \n",
      "120 80% (0m 55s) 0.3149 \n",
      "130 86% (0m 58s) 0.2555 \n",
      "140 93% (1m 3s) 0.4467 \n",
      "150 100% (1m 9s) 0.2900 \n",
      "Epoch  3  Train_auc : 0.836797877046  , Valid_auc :  0.5  ,& Test_auc :  0.512820512821\n",
      "10 6% (0m 8s) 0.3622 \n",
      "20 13% (0m 9s) 0.3657 \n",
      "30 20% (0m 13s) 1.2465 \n",
      "40 26% (0m 17s) 0.5580 \n",
      "50 33% (0m 22s) 0.4564 \n",
      "60 40% (0m 26s) 0.3750 \n",
      "70 46% (0m 34s) 2.4582 \n",
      "80 53% (0m 36s) 0.4324 \n",
      "90 60% (0m 39s) 0.3454 \n",
      "100 66% (0m 47s) 0.4688 \n",
      "110 73% (0m 56s) 0.3464 \n",
      "120 80% (1m 0s) 0.2684 \n",
      "130 86% (1m 4s) 0.4300 \n",
      "140 93% (1m 7s) 0.4159 \n",
      "150 100% (1m 11s) 0.4245 \n",
      "Epoch  4  Train_auc : 0.78726227333  , Valid_auc :  0.558823529412  ,& Test_auc :  0.525641025641\n",
      "10 6% (0m 5s) 0.4057 \n",
      "20 13% (0m 11s) 0.4253 \n",
      "30 20% (0m 14s) 0.3632 \n",
      "40 26% (0m 17s) 0.2997 \n",
      "50 33% (0m 26s) 0.6869 \n",
      "60 40% (0m 27s) 0.4718 \n",
      "70 46% (0m 31s) 2.5370 \n",
      "80 53% (0m 32s) 0.3056 \n",
      "90 60% (0m 36s) 0.2575 \n",
      "100 66% (0m 45s) 0.3661 \n",
      "110 73% (0m 49s) 0.3197 \n",
      "120 80% (0m 53s) 0.3884 \n",
      "130 86% (1m 3s) 0.6207 \n",
      "140 93% (1m 5s) 0.6930 \n",
      "150 100% (1m 9s) 1.7316 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1f969ebf7381>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcurrent_loss_la\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_losses_la\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtrain_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_auc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_sl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtest_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_auc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_sl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mvalid_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_auc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_sl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" Train_auc :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" , Valid_auc : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_auc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" ,& Test_auc : \"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtest_auc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-bd065aceb378>\u001b[0m in \u001b[0;36mcalculate_auc\u001b[1;34m(test_model, dataset, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0my_hat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mlabelVec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c6584760e190>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     88\u001b[0m           \u001b[1;31m#  print (\"b,seq,features\",b,seq,features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# get alpha coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0moutputs1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRNN1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# [b x seq x 128*2] , the hidden on 2*b*128\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m            \u001b[1;31m# print('outputs1', outputs1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# [b*seq x 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, weight, hidden)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mnexth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;31m# hack to handle LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\lgindybekhet\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[1;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0minputgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_i\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mnewgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_n\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresetgate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mh_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m     \u001b[0mhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewgate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minputgate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnewgate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "batch_size=200\n",
    "current_loss_l=[]\n",
    "all_losses_l=[]\n",
    "\n",
    "for ep in range(epochs):\n",
    "    #print (\"Epoch \", ep )\n",
    "    current_loss_la,all_losses_la = run_model_train(train_sl,batch_size)\n",
    "    train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "    test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "    valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "    print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc)\n",
    "    current_loss_l.append(current_loss_la)\n",
    "    all_losses_l.append (all_losses_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.580540269613266,\n",
       "  0.6151448905467987,\n",
       "  0.5401335477828979,\n",
       "  0.6278171360492706,\n",
       "  0.6401678860187531,\n",
       "  0.3816168010234833,\n",
       "  0.39800177812576293,\n",
       "  0.3970666706562042,\n",
       "  0.5435400366783142,\n",
       "  0.4139790594577789,\n",
       "  0.49660996794700624,\n",
       "  0.5397546768188477,\n",
       "  1.2314470291137696,\n",
       "  4.3527842402458194,\n",
       "  1.3446409463882447,\n",
       "  0.6595134794712066,\n",
       "  1.5113649427890778,\n",
       "  0.8701830089092255,\n",
       "  0.8862375855445862,\n",
       "  1.579005056619644,\n",
       "  1.933153223991394,\n",
       "  0.4697125911712646,\n",
       "  0.9309005022048951,\n",
       "  1.4198203682899475,\n",
       "  1.7455278217792511,\n",
       "  1.124794602394104,\n",
       "  1.342689448595047,\n",
       "  0.7003886222839355,\n",
       "  0.6690422296524048,\n",
       "  0.6423026204109192],\n",
       " [0.44635125398635866,\n",
       "  0.6875577390193939,\n",
       "  1.2218059182167054,\n",
       "  0.4167063057422638,\n",
       "  0.6810052275657654,\n",
       "  0.5981203734874725,\n",
       "  0.5208072662353516,\n",
       "  0.619415020942688,\n",
       "  0.5455323576927185,\n",
       "  1.3128721117973328,\n",
       "  0.4281072080135345,\n",
       "  0.40259002447128295,\n",
       "  1.0054753065109252,\n",
       "  0.7001903891563416,\n",
       "  0.5961590051651001,\n",
       "  0.3993136763572693,\n",
       "  0.38240917325019835,\n",
       "  0.5783710181713104,\n",
       "  1.3694444835186004,\n",
       "  1.102413272857666,\n",
       "  1.2700321674346924,\n",
       "  0.9525881767272949,\n",
       "  1.202620130777359,\n",
       "  1.6906551480293275,\n",
       "  0.7896850228309631,\n",
       "  0.9083403646945953,\n",
       "  0.42035548090934755,\n",
       "  0.4826101243495941,\n",
       "  1.0141352593898774,\n",
       "  1.5088062584400177],\n",
       " [0.5877232849597931,\n",
       "  0.6706332087516784,\n",
       "  0.5598883390426636,\n",
       "  0.4203444838523865,\n",
       "  0.5309376299381257,\n",
       "  0.35379706621170043,\n",
       "  0.35715351104736326,\n",
       "  0.39168850183486936,\n",
       "  0.3583198606967926,\n",
       "  0.30609560310840606,\n",
       "  0.33755120635032654,\n",
       "  0.5693103909492493,\n",
       "  1.2485669374465942,\n",
       "  3.5080024182796476,\n",
       "  0.461211758852005,\n",
       "  0.598219758272171,\n",
       "  1.9737664759159088,\n",
       "  1.2857471346855163,\n",
       "  1.2063791871070861,\n",
       "  0.9029633522033691,\n",
       "  0.9282551348209381,\n",
       "  1.5859971642494202,\n",
       "  0.43393818140029905,\n",
       "  1.1894571483135223,\n",
       "  0.6785532891750335,\n",
       "  0.35082708597183226,\n",
       "  1.0106865286827087,\n",
       "  0.4600160837173462,\n",
       "  1.1083835422992707,\n",
       "  1.8962870419025422],\n",
       " [0.9973266124725342,\n",
       "  0.6905848741531372,\n",
       "  0.4250215172767639,\n",
       "  0.48146532773971557,\n",
       "  1.4166395366191864,\n",
       "  0.6441320657730103,\n",
       "  1.5124394774436951,\n",
       "  0.44481725096702573,\n",
       "  0.968551117181778,\n",
       "  0.34353628754615784,\n",
       "  0.6664139151573181,\n",
       "  1.1669719517230988,\n",
       "  0.3689593255519867,\n",
       "  0.4171406090259552,\n",
       "  0.33549954295158385,\n",
       "  0.3635440647602081,\n",
       "  4.460710620880127,\n",
       "  0.45264793038368223,\n",
       "  0.5735940277576447,\n",
       "  0.3446184813976288,\n",
       "  0.3493751585483551,\n",
       "  0.3534730076789856,\n",
       "  0.3475589632987976,\n",
       "  0.30238263607025145,\n",
       "  0.3366544544696808,\n",
       "  0.38845091462135317,\n",
       "  0.3670214116573334,\n",
       "  0.3803750157356262,\n",
       "  0.36773018836975097,\n",
       "  0.27327350676059725],\n",
       " [0.7782895714044571,\n",
       "  0.541270089149475,\n",
       "  0.3600729048252106,\n",
       "  0.44508516788482666,\n",
       "  0.4643340766429901,\n",
       "  0.7058805048465728,\n",
       "  0.5024193525314331,\n",
       "  0.5357662498950958,\n",
       "  0.4531632363796234,\n",
       "  0.3187318921089172,\n",
       "  0.34974235892295835,\n",
       "  0.6696959674358368,\n",
       "  0.6976498126983642,\n",
       "  1.331100195646286,\n",
       "  0.3583274006843567,\n",
       "  0.4247873663902283,\n",
       "  0.5113215923309327,\n",
       "  0.3671832501888275,\n",
       "  0.7311375796794891,\n",
       "  0.5041545152664184,\n",
       "  0.3898122787475586,\n",
       "  4.626765811443329,\n",
       "  0.4216365098953247,\n",
       "  0.29398744702339175,\n",
       "  0.32950506210327146,\n",
       "  0.3705127418041229,\n",
       "  0.2905018627643585,\n",
       "  0.31521274149417877,\n",
       "  0.6649474084377289,\n",
       "  0.35368375182151796]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01959603469\n",
      "0.808482508858\n",
      "0.875690043668\n",
      "0.684696993132\n",
      "0.636889290015\n"
     ]
    }
   ],
   "source": [
    "for x_ep in all_losses_l:\n",
    "    y_ep = np.mean(x_ep)\n",
    "    print (y_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
