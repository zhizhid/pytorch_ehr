{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is playing with pytorch framework for EHR modeling. In general, a patient's health record can be represented as a sequence of visits. Each visit has certain features, and can be represented as a list of medical codes.\n",
    "\n",
    "For simplicity, we are starting with the data structure that a patient's health record is a list of list, following the line of work from Jimeng Sun's lab. We will use codes from Ed Choi to manipulate the data. \n",
    "\n",
    "The core model is an GRU.\n",
    "\n",
    "# todos:\n",
    "* dropout\n",
    "* L1/L2 regularization\n",
    "* validation and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import sklearn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data: load the input file containing list of list of list, and corresponding label file\n",
    "# and output the splitted training, valid and Test sets\n",
    "\n",
    "def data_load_split_VT(seqFile = 'pytorch_ehr-master/pytorch_ehr-master/data/cerner/hospital_data/h143.visits', labelFile = 'pytorch_ehr-master/pytorch_ehr-master/data/cerner/hospital_data/h143.labels' , test_r=0.2 , valid_r=0.1):\n",
    "\n",
    "    set_x = pickle.load(open(seqFile, 'rb'), encoding='bytes')\n",
    "    set_y = pickle.load(open(labelFile, 'rb'),encoding='bytes')\n",
    "    merged_set = [[set_y[i],set_x[i]] for i in range(len(set_x))] # merge the two lists\n",
    "\n",
    "    # set random seed\n",
    "    random.seed( 3 )\n",
    "    \n",
    "    dataSize = len(merged_set)\n",
    "    nTest = int(test_r * dataSize)\n",
    "    nValid = int(valid_r * dataSize)\n",
    "    \n",
    "    random.shuffle(merged_set)\n",
    "\n",
    "    test_set = merged_set[:nTest]\n",
    "    valid_set = merged_set[nTest:nTest+nValid]\n",
    "    train_set = merged_set[nTest+nValid:]\n",
    "\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sl , valid_sl , test_sl = data_load_split_VT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8545"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,embed_dim, n_layers=1,dropout_r=0.1):\n",
    "        super(EHR_RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "\n",
    "#        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.embedBag = nn.EmbeddingBag(input_size, self.embed_dim,mode= 'sum')\n",
    "        self.gru = nn.GRU(self.embed_dim, hidden_size, dropout= dropout_r )\n",
    "        self.out = nn.Linear(self.hidden_size,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "        \n",
    "    def EmbedPatient_MB(self, seq_mini_batch): # x is a ehr_seq_tensor\n",
    "        \n",
    "        lp= len(max(seq_mini_batch, key=lambda xmb: len(xmb[1]))[1]) # max number of visitgs within mb ??? verify again\n",
    "        #print ('longest',lp)\n",
    "        tb= torch.FloatTensor(len(seq_mini_batch),lp,self.embed_dim) \n",
    "        lbt1= torch.FloatTensor(len(seq_mini_batch),1)\n",
    "\n",
    "        for pt in range(len(seq_mini_batch)):\n",
    "              \n",
    "            lbt ,pt_visits =seq_mini_batch[pt]\n",
    "            lbt1[pt] = torch.FloatTensor([[float(lbt)]])\n",
    "            ml=(len(max(pt_visits, key=len))) ## getting the visit with max no. of codes ##the max number of visits for pts within the minibatch\n",
    "            txs= torch.LongTensor(len(pt_visits),ml)\n",
    "            \n",
    "            b=0\n",
    "            for i in pt_visits:\n",
    "                pd=(0, ml-len(i))\n",
    "                txs[b] = F.pad(torch.from_numpy(np.asarray(i)).view(1,-1),pd,\"constant\", 0).data\n",
    "                b=b+1\n",
    "            \n",
    "            if use_cuda:\n",
    "                txs=txs.cuda()\n",
    "                \n",
    "            emb_bp= self.embedBag(Variable(txs)) ### embed will be num_of_visits*max_num_codes*embed_dim \n",
    "            #### the embed Bag dim will be num_of_visits*embed_dim\n",
    "            \n",
    "            zp= nn.ZeroPad2d((0,0,0,(lp-len(pt_visits))))\n",
    "            xzp= zp(emb_bp)\n",
    "            tb[pt]=xzp.data\n",
    "\n",
    "        tb= tb.permute(1, 0, 2) ### as my final input need to be seq_len x batch_size x input_size\n",
    "        emb_m=Variable(tb)\n",
    "        label_tensor = Variable(lbt1)\n",
    "\n",
    "        if use_cuda:\n",
    "                label_tensor = label_tensor.cuda()\n",
    "                emb_m = emb_m.cuda()\n",
    "                \n",
    "        return emb_m , label_tensor\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        x_in , lt = self.EmbedPatient_MB(input)\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "                output, hidden = self.gru(x_in, hidden) # input (seq_len, batch, input_size) need to check torch.nn.utils.rnn.pack_padded_sequence() \n",
    "                                                          #h_0 (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n",
    "\n",
    "        output = self.sigmoid(self.out(output[0]))\n",
    "        return output, lt\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = EHR_RNN(input_size=20000, hidden_size=128 ,embed_dim=128, dropout_r=0.1)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train (mini_batch, criterion, optimizer):  \n",
    "    \n",
    "    hidden = model.initHidden()\n",
    "    model.zero_grad()\n",
    "    output , label_tensor = model(mini_batch,hidden)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    return output, loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variableFromEHRSeq(ehr_seq):\n",
    "    # ehr_seq is a list of list\n",
    "    result = []\n",
    "    if use_cuda:\n",
    "        for i in range(len(ehr_seq)):\n",
    "            result.append( Variable(torch.LongTensor([int(v) for v in ehr_seq[i]])).cuda() )\n",
    "    # if use_cuda:\n",
    "    #     return result.cuda()\n",
    "    else:\n",
    "        for i in range(len(ehr_seq)):\n",
    "            result.append( Variable(torch.LongTensor([int(v) for v in ehr_seq[i]])) )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train(dataset,batch_size,learning_rate = 0.001 ):\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    #optimizer = optim.Adam(model.parameters(), weight_decay=0)\n",
    "    optimizer = optim.RMSprop (model.parameters())\n",
    "\n",
    "    dataset.sort(key=lambda pt:len(pt[1])) \n",
    "   \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    #print('number of Batches',n_batches)\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "            # Print iter number, loss, name and guess\n",
    "            #if iter % print_every == 0:\n",
    "            #       print('%d %d%% (%s) %.4f ' % ( iter, iter/ n_batches * 100, timeSince(start), loss))\n",
    "\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "    hidden = model.initHidden()\n",
    "\n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = model(batch,hidden)\n",
    "            y_hat.append(output.cpu().data.numpy()[0][0])\n",
    "            labelVec.append(label_t.cpu().data.numpy()[0][0])\n",
    "    #print (labelVec, y_hat)\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train_auc : 0.741486068111  , Valid_auc :  0.541176470588  ,& Test_auc :  0.75641025641\n",
      "Epoch  1  Train_auc : 0.752543122512  , Valid_auc :  0.447058823529  ,& Test_auc :  0.416666666667\n",
      "Epoch  2  Train_auc : 0.693940734188  , Valid_auc :  0.447058823529  ,& Test_auc :  0.576923076923\n",
      "Epoch  3  Train_auc : 0.767801857585  , Valid_auc :  0.564705882353  ,& Test_auc :  0.455128205128\n",
      "Epoch  4  Train_auc : 0.723131357806  , Valid_auc :  0.341176470588  ,& Test_auc :  0.525641025641\n",
      "Epoch  5  Train_auc : 0.700132684653  , Valid_auc :  0.294117647059  ,& Test_auc :  0.49358974359\n",
      "Epoch  6  Train_auc : 0.764927023441  , Valid_auc :  0.364705882353  ,& Test_auc :  0.448717948718\n",
      "Epoch  7  Train_auc : 0.795223352499  , Valid_auc :  0.411764705882  ,& Test_auc :  0.519230769231\n",
      "Epoch  8  Train_auc : 0.794117647059  , Valid_auc :  0.694117647059  ,& Test_auc :  0.538461538462\n",
      "Epoch  9  Train_auc : 0.84077841663  , Valid_auc :  0.429411764706  ,& Test_auc :  0.512820512821\n",
      "Epoch  10  Train_auc : 0.730429013711  , Valid_auc :  0.329411764706  ,& Test_auc :  0.403846153846\n",
      "Epoch  11  Train_auc : 0.781512605042  , Valid_auc :  0.488235294118  ,& Test_auc :  0.608974358974\n",
      "Epoch  12  Train_auc : 0.753206545776  , Valid_auc :  0.617647058824  ,& Test_auc :  0.583333333333\n",
      "Epoch  13  Train_auc : 0.763379035825  , Valid_auc :  0.376470588235  ,& Test_auc :  0.467948717949\n",
      "Epoch  14  Train_auc : 0.792790800531  , Valid_auc :  0.388235294118  ,& Test_auc :  0.576923076923\n",
      "Epoch  15  Train_auc : 0.823529411765  , Valid_auc :  0.570588235294  ,& Test_auc :  0.551282051282\n",
      "Epoch  16  Train_auc : 0.796550199027  , Valid_auc :  0.341176470588  ,& Test_auc :  0.410256410256\n",
      "Epoch  17  Train_auc : 0.800973020787  , Valid_auc :  0.452941176471  ,& Test_auc :  0.49358974359\n",
      "Epoch  18  Train_auc : 0.773772666962  , Valid_auc :  0.447058823529  ,& Test_auc :  0.269230769231\n",
      "Epoch  19  Train_auc : 0.816452896948  , Valid_auc :  0.547058823529  ,& Test_auc :  0.49358974359\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "batch_size=200\n",
    "current_loss_l=[]\n",
    "all_losses_l=[]\n",
    "\n",
    "for ep in range(epochs):\n",
    "    #print (\"Epoch \", ep )\n",
    "    current_loss_la,all_losses_la = run_model_train(train_sl,batch_size)\n",
    "    train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "    test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "    valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "    print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc)\n",
    "    current_loss_l.append(current_loss_la)\n",
    "    all_losses_l.append (all_losses_la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-39d8c6676429>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
