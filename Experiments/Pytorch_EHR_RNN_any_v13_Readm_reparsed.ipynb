{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is playing with pytorch framework for EHR modeling. In general, a patient's health record can be represented as a sequence of visits. Each visit has certain features, and can be represented as a list of medical codes.\n",
    "\n",
    "For simplicity, we are starting with the data structure that a patient's health record is a list of list, following the line of work from Jimeng Sun's lab. We will use codes from Ed Choi to manipulate the data. \n",
    "\n",
    "The core model is an RNN , either LSTM, GRU or Vanilla RNN.\n",
    "\n",
    "# todos:\n",
    "* None for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import sklearn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "# for windows only    \n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]='PCI_BUS_ID'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "#%env CUDA_VISIBLE_DEVICES=1\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12629 1803 3607\n"
     ]
    }
   ],
   "source": [
    "merged_set= pickle.load(open('/data/projects/py_ehr_2/Data/Readm_h_143cl2_combined_tp.all', 'rb'), encoding='bytes')\n",
    "\n",
    "dataSize = len(merged_set)\n",
    "nTest = int(0.2 * dataSize)\n",
    "nValid = int(0.1 * dataSize)\n",
    "    \n",
    "random.shuffle(merged_set)\n",
    "\n",
    "test_sl = merged_set[:nTest]\n",
    "valid_sl = merged_set[nTest:nTest+nValid]\n",
    "train_sl = merged_set[nTest+nValid:]\n",
    "\n",
    "print (len(train_sl),len(valid_sl),len(test_sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0], [1, 55, 69, 56, 19, 57, 429, 7, 5, 61, 62, 154, 63, 13, 36, 11]], [[21], [61, 56, 4, 57, 60, 429, 7, 55, 36, 62, 754, 63, 13, 11, 69, 1]], [[1], [61, 56, 57, 60, 429, 7, 55, 4, 62, 63, 13, 36, 11, 1, 69, 754]], [[29], [61, 56, 4, 57, 5, 429, 7, 55, 62, 1, 68, 63, 1407, 13, 36, 11, 69, 65]], [[42], [60, 7, 429, 5, 61, 57, 4, 56, 55, 1, 62, 536, 754, 68, 63, 13, 36, 11, 69]], [[14], [61, 56, 4, 57, 143, 429, 7, 55, 62, 140, 63, 13, 36, 11, 69, 1]], [[5], [61, 4, 57, 5, 429, 7, 55, 56, 62, 63, 13, 36, 11, 69, 1, 754]], [[7], [61, 56, 4, 57, 5, 429, 7, 55, 62, 68, 63, 13, 36, 11, 69, 1]], [[17], [55, 56, 4, 57, 5, 429, 7, 61, 1, 68, 63, 13, 36, 11, 69, 62]], [[4], [62, 4, 5, 429, 7, 55, 61, 56, 70, 11, 63, 1, 69, 57, 36, 13, 338]], [[21], [61, 56, 4, 57, 5, 429, 7, 55, 63, 62, 68, 13, 36, 11, 69, 1]], [[1], [7, 429, 60, 61, 15, 56, 55, 57, 62, 63, 13, 36, 11, 69, 1, 205]], [[7], [55, 56, 15, 57, 102, 429, 7, 61, 62, 643, 63, 13, 36, 11, 69, 1]], [[71], [61, 56, 4, 57, 5, 429, 7, 55, 11, 62, 63, 13, 36, 69, 1, 68]], [[509], [429, 265, 553, 27, 554, 7, 326, 218, 102, 22, 56, 555, 30, 537, 848, 919, 545, 272, 36, 13, 350, 11, 203, 182, 1519, 236, 557]], [[44], [42, 56, 4, 326, 60, 116, 429, 7, 167, 27, 72, 29, 84, 1519, 13, 36, 11, 272, 1, 168]], [[21], [122, 163, 7, 42, 56, 60, 4167, 4, 27, 429, 29, 166, 1335, 13, 36, 11, 272, 1, 1518]], [[7], [2, 56, 4, 60, 429, 7, 12, 72, 13, 10, 11, 8, 1]], [[11], [2, 56, 4, 5, 429, 7, 8, 65, 13, 10, 11, 1, 12]], [[1], [429, 29, 42, 550, 7, 5, 549, 326, 4, 56, 552, 68, 27, 1329, 488, 133, 1, 289, 11, 13, 36, 477, 1142, 639, 75, 118]], [[37], [7, 29, 42, 27, 257, 56, 429, 218, 5, 326, 4, 848, 258, 30, 545, 660, 557, 1, 289, 36, 13, 11, 1020, 301, 537, 826, 75]], [[11], [27, 490, 5, 160, 429, 7, 161, 289, 42, 29, 162, 13, 36, 11, 104, 56]]]\n",
      "[[27, 490, 5, 160, 429, 7, 161, 289, 42, 29, 162, 13, 36, 11, 104, 56]]\n"
     ]
    }
   ],
   "source": [
    "x=train_sl[2]\n",
    "print(x[-1])\n",
    "print(x[-1][-1][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom collections import OrderedDict\\ndef load_params(model):\\n    params = OrderedDict()\\n    weights = np.load(model)\\n    for k,v in weights.iteritems():\\n        params[k] = v\\n    return params\\n\\nparam= load_params(\"/data/projects/cerner_dev/cl2_hosp_retain_out/h50_cl2_all_hosp_143_cl2m.out.0.npz\")\\nemb_pretrain = param[\\'W_emb\\']\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load preTrained Emb from previous model (RETAIN)\n",
    "'''\n",
    "from collections import OrderedDict\n",
    "def load_params(model):\n",
    "    params = OrderedDict()\n",
    "    weights = np.load(model)\n",
    "    for k,v in weights.iteritems():\n",
    "        params[k] = v\n",
    "    return params\n",
    "\n",
    "param= load_params(\"/data/projects/cerner_dev/cl2_hosp_retain_out/h50_cl2_all_hosp_143_cl2m.out.0.npz\")\n",
    "emb_pretrain = param['W_emb']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHR_RNN(nn.Module):\n",
    "    def __init__(self, input_size,embed_dim, hidden_size, n_layers=1,dropout_r=0.1,cell_type='LSTM',bi=False , preTrainEmb=''):\n",
    "        super(EHR_RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_r = dropout_r\n",
    "        self.cell_type = cell_type\n",
    "        self.preTrainEmb=preTrainEmb\n",
    "        if bi: self.bi=2 \n",
    "        else: self.bi=1\n",
    "              \n",
    "        if len(self.preTrainEmb)>0:\n",
    "            emb_t= torch.FloatTensor(np.asmatrix(self.preTrainEmb))\n",
    "            self.embed= nn.Embedding.from_pretrained(emb_t)#,freeze=False)  \n",
    "        else:\n",
    "            self.embed= nn.Embedding(input_size, self.embed_dim,padding_idx=0)#,scale_grad_by_freq=True)\n",
    "        \n",
    "        if self.cell_type == \"GRU\":\n",
    "            cell = nn.GRU\n",
    "        elif self.cell_type == \"RNN\":\n",
    "            cell = nn.RNN\n",
    "        elif self.cell_type == \"LSTM\":\n",
    "            cell = nn.LSTM\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "      \n",
    "        self.rnn_c = cell(self.embed_dim+1, hidden_size,num_layers=n_layers, dropout= dropout_r , bidirectional=bi , batch_first=True )\n",
    "        self.out = nn.Linear(self.hidden_size*self.bi,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #hl=int(self.hidden_size/4)\n",
    "        #print (hl)\n",
    "        #self.hid1_l= nn.Linear(3,hl)\n",
    "        #self.hid2_l= nn.Linear(hl,self.hidden_size)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.hid_l= nn.Linear(self.embed_dim,self.hidden_size)\n",
    "        \n",
    "    def EmbedPatient_MB(self, input): # x is a ehr_seq_tensor\n",
    "        \n",
    "        mb=[]\n",
    "        mtd=[]\n",
    "        lbt=[]\n",
    "        seq_l=[]\n",
    "        self.bsize=len(input)\n",
    "        lp= len(max(input, key=lambda xmb: len(xmb[-1]))[-1])\n",
    "        #print (max(input, key=lambda xmb: len(xmb[-1])),lp) #verified\n",
    "        llv=0\n",
    "        for x in input:\n",
    "            lv= len(max(x[-1], key=lambda xmb: len(xmb[1]))[1])\n",
    "            #print(max(x[-1], key=lambda xmb: len(xmb[1:])),lv) #verified  \n",
    "            if llv < lv:\n",
    "                llv=lv             \n",
    "        #print (llv)\n",
    "        for pt in input:\n",
    "            sk,label,ehr_seq_l = pt\n",
    "            lpx=len(ehr_seq_l)\n",
    "            seq_l.append(lpx)\n",
    "            label_tensor = Variable(torch.FloatTensor([[float(label)]]))\n",
    "            if use_cuda:\n",
    "                label_tensor = label_tensor.cuda()\n",
    "            lbt.append(label_tensor)\n",
    "            if use_cuda:\n",
    "                    flt_typ=torch.cuda.FloatTensor\n",
    "                    lnt_typ=torch.cuda.LongTensor\n",
    "            else: \n",
    "                lnt_typ=torch.LongTensor\n",
    "                flt_typ=torch.FloatTensor\n",
    "            ml=(len(max(ehr_seq_l, key=len)))\n",
    "            ehr_seq_tl=[]\n",
    "            time_dim=[]\n",
    "            for ehr_seq in ehr_seq_l:\n",
    "                #print (ehr_seq,ehr_seq[1])#verified\n",
    "                #print(n_ehr_seq)\n",
    "                pd=(0, (llv -len(ehr_seq[1])))\n",
    "                time_dim.append(Variable(torch.from_numpy(np.asarray(ehr_seq[0],dtype=int)).type(flt_typ)))\n",
    "                result = F.pad(torch.from_numpy(np.asarray(ehr_seq[1],dtype=int)).type(lnt_typ),pd,\"constant\", 0)\n",
    "                ehr_seq_tl.append(result)\n",
    "            ehr_seq_t= Variable(torch.stack(ehr_seq_tl,0)) \n",
    "            time_dim_v= Variable(torch.stack(time_dim,0))\n",
    "            lpp= lp-lpx\n",
    "            zp= nn.ZeroPad2d((0,0,0,lpp))\n",
    "            ehr_seq_t= zp(ehr_seq_t)\n",
    "            time_dim_pv= zp(time_dim_v)\n",
    "            mb.append(ehr_seq_t)\n",
    "            mtd.append(time_dim_pv)\n",
    "            #print('ehr_seq_t',ehr_seq_t) #verified\n",
    "            \n",
    "        mb_t= Variable(torch.stack(mb,0)) \n",
    "        mtd_t= Variable(torch.stack(mtd,0))\n",
    "        if use_cuda:\n",
    "            mb_t.cuda()\n",
    "            mtd_t.cuda()\n",
    "        embedded = self.embed(mb_t)\n",
    "        #print(mb_t,embedded) #verified\n",
    "        embedded = torch.sum(embedded, dim=2) \n",
    "        lbt_t= Variable(torch.stack(lbt,0))\n",
    "        #dem_t= Variable(torch.stack(demt,0))\n",
    "        #if use_cuda: dem_t.cuda()\n",
    "        #dem_emb=self.embed(dem_t)\n",
    "        #dem_emb = torch.sum(dem_emb, dim=1) \n",
    "        #print ('embedded',embedded.shape,embedded,'time_dim_pv',mtd_t.shape,mtd_t)\n",
    "        out_emb= torch.cat((embedded,mtd_t),dim=2)\n",
    "        #print ('out_emb with time',out_emb.shape,out_emb)\n",
    "        return out_emb, lbt_t,seq_l #,dem_emb\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        h_0 = Variable(torch.rand(self.n_layers*self.bi,self.bsize, self.hidden_size))\n",
    "        \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            result = (h_0,h_0)\n",
    "        else: \n",
    "            result = h_0\n",
    "            \n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \n",
    "        x_in , lt ,x_lens = self.EmbedPatient_MB(input)\n",
    "        #print (dem_t.size())\n",
    "        x_inp = nn.utils.rnn.pack_padded_sequence(x_in,x_lens,batch_first=True)\n",
    "        \n",
    "        h_0= self.init_hidden()\n",
    "        #h_0 = self.sigmoid(self.hid2_l(self.sigmoid(self.hid1_l(dem_t)))).unsqueeze(0)\n",
    "        #h_0 = self.sigmoid(self.hid_l(dem_t)).unsqueeze(0)\n",
    "        #print (h_0)\n",
    "        output, hidden = self.rnn_c(x_inp,h_0) \n",
    "        if self.cell_type == \"LSTM\":\n",
    "            hidden=hidden[0]\n",
    "        if self.bi==2:\n",
    "            output = self.sigmoid(self.out(torch.cat((hidden[-2],hidden[-1]),1)))\n",
    "        else:\n",
    "            output = self.sigmoid(self.out(hidden[-1]))\n",
    "        return output.squeeze(), lt.squeeze()\n",
    "\n",
    "# GRU: new data format : bestValidAuc 0.831529 has a TestAuc of 0.833067 at epoch 16    \n",
    "## versus bestValidAuc 0.821507 has a TestAuc of 0.837944 at epoch 2 with default h_0\n",
    "# h_0 random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_pretrain=''                       \n",
    "model = EHR_RNN(input_size=18000, hidden_size=64 ,embed_dim=64, dropout_r=0, cell_type='GRU',bi=False , n_layers=1, preTrainEmb=emb_pretrain)\n",
    "bmodel_pth = './best_model_GRUPreReadm.pth'\n",
    "bmodel_st = './best_model_GRUPreReadm.st'\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (tmodel,mini_batch, criterion, optimizer):  \n",
    "    tmodel.train()\n",
    "    model.zero_grad()\n",
    "    output , label_tensor = tmodel(mini_batch)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train(tmodel,dataset,batch_size,learning_rate = 0.03, l2=0.001,epsl=1e-04 ):\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)#, weight_decay=l2)\n",
    "    #optimizer = optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    #optimizer = optim.ASGD(model.parameters(), lr=learning_rate, weight_decay=l2 )\n",
    "    #optimizer = optim.SparseAdam (model.parameters(),lr=learning_rate) #'''lr=learning_rate,''' \n",
    "    optimizer = optim.Adagrad (tmodel.parameters(),lr=learning_rate, weight_decay=l2) #'''lr=learning_rate,''' \n",
    "    #optimizer = optim.Adamax(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=l2 ,eps=epsl) ### Beta defaults (0.9, 0.999)\n",
    "    #optimizer = optim.RMSprop (model.parameters(),lr=learning_rate, weight_decay=l2 ,eps=epsl)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=learning_rate)\n",
    "    dataset.sort(key=lambda pt:len(pt[-1]),reverse=True) \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    #print('number of Batches',n_batches)\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(tmodel,batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "            # Print iter number, loss, name and guess\n",
    "            #if iter % print_every == 0:\n",
    "               #print('%d %d%% (%s) %.4f ' % ( iter, iter/ n_batches * 100, timeSince(start), loss))\n",
    "\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "    test_model.eval()\n",
    "    dataset.sort(key=lambda pt:len(pt[-1]),reverse=True) \n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "    \n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = test_model(batch)\n",
    "            y_hat.extend(output.cpu().data.view(-1).numpy())\n",
    "            labelVec.extend(label_t.cpu().data.view(-1).numpy())\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train_auc : 0.7235692920655318  , Valid_auc :  0.6831921196010972  ,& Test_auc :  0.6970752327651648  Avg Loss:  0.6578739070892334 Train Time (1m 42s) Eval Time (2m 22s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgindybekhet/.local/lib/python3.5/site-packages/torch/serialization.py:193: UserWarning:\n",
      "\n",
      "Couldn't retrieve source code for container of type EHR_RNN. It won't be checked for correctness upon loading.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Train_auc : 0.7419711697295588  , Valid_auc :  0.7125717175592488  ,& Test_auc :  0.718419382701365  Avg Loss:  0.6316814613342285 Train Time (1m 40s) Eval Time (2m 0s)\n",
      "Epoch  2  Train_auc : 0.7597594597662243  , Valid_auc :  0.7246880301743144  ,& Test_auc :  0.7335426574741732  Avg Loss:  0.6272083878517151 Train Time (0m 51s) Eval Time (1m 13s)\n",
      "Epoch  3  Train_auc : 0.7672895735327203  , Valid_auc :  0.7205475322682305  ,& Test_auc :  0.7374369707865784  Avg Loss:  0.6279473633766175 Train Time (0m 52s) Eval Time (1m 14s)\n",
      "Epoch  4  Train_auc : 0.7716074128611068  , Valid_auc :  0.7196930251793094  ,& Test_auc :  0.7382253613923999  Avg Loss:  0.6171997375488282 Train Time (0m 52s) Eval Time (1m 12s)\n",
      "Epoch  5  Train_auc : 0.7660005416728316  , Valid_auc :  0.7210644716879131  ,& Test_auc :  0.735150887126254  Avg Loss:  0.6179316773414613 Train Time (0m 53s) Eval Time (1m 12s)\n",
      "Epoch  6  Train_auc : 0.7771484244363867  , Valid_auc :  0.7248026287926538  ,& Test_auc :  0.7464938102619064  Avg Loss:  0.6173680012226105 Train Time (0m 51s) Eval Time (1m 11s)\n",
      "Epoch  7  Train_auc : 0.7818050499717255  , Valid_auc :  0.7283583000540607  ,& Test_auc :  0.7464891397014453  Avg Loss:  0.6072317833900451 Train Time (0m 52s) Eval Time (1m 12s)\n",
      "Epoch  8  Train_auc : 0.7798143516444761  , Valid_auc :  0.7223761276130354  ,& Test_auc :  0.7443546935707556  Avg Loss:  0.6076817853450774 Train Time (0m 52s) Eval Time (1m 12s)\n",
      "Epoch  9  Train_auc : 0.7927591668879299  , Valid_auc :  0.7276339620479271  ,& Test_auc :  0.751143197515511  Avg Loss:  0.6062478058338165 Train Time (0m 52s) Eval Time (1m 12s)\n",
      "Epoch  10  Train_auc : 0.7858007485117932  , Valid_auc :  0.7284697845795103  ,& Test_auc :  0.7501767028707755  Avg Loss:  0.6191860814094543 Train Time (0m 51s) Eval Time (1m 12s)\n",
      "Epoch  11  Train_auc : 0.7906601213266069  , Valid_auc :  0.728986723999193  ,& Test_auc :  0.7509959191756399  Avg Loss:  0.613887934923172 Train Time (0m 52s) Eval Time (1m 13s)\n",
      "Epoch  12  Train_auc : 0.7949605931748209  , Valid_auc :  0.7255674500063527  ,& Test_auc :  0.7523816744644267  Avg Loss:  0.6050824682712554 Train Time (0m 52s) Eval Time (1m 12s)\n",
      "Epoch  13  Train_auc : 0.8004769026448227  , Valid_auc :  0.7256459251471721  ,& Test_auc :  0.7593773956083032  Avg Loss:  0.6017515950202942 Train Time (0m 52s) Eval Time (1m 12s)\n",
      "Epoch  14  Train_auc : 0.7908571301170094  , Valid_auc :  0.7273138332988707  ,& Test_auc :  0.7553551089392658  Avg Loss:  0.5963010537624359 Train Time (0m 51s) Eval Time (1m 12s)\n",
      "Epoch  15  Train_auc : 0.7975725431159414  , Valid_auc :  0.7327684784043887  ,& Test_auc :  0.7582225216916396  Avg Loss:  0.5969541001319886 Train Time (0m 53s) Eval Time (1m 14s)\n",
      "Epoch  16  Train_auc : 0.8002455515942106  , Valid_auc :  0.7268803515686308  ,& Test_auc :  0.7549596681535655  Avg Loss:  0.5992905569076538 Train Time (0m 53s) Eval Time (1m 14s)\n",
      "Epoch  17  Train_auc : 0.8059489003382796  , Valid_auc :  0.7276968667242982  ,& Test_auc :  0.7524802232901545  Avg Loss:  0.5908234949111939 Train Time (0m 53s) Eval Time (1m 14s)\n",
      "Epoch  18  Train_auc : 0.7938865076340906  , Valid_auc :  0.7195074252430861  ,& Test_auc :  0.7478773859558117  Avg Loss:  0.5945139122009276 Train Time (0m 52s) Eval Time (1m 14s)\n",
      "Epoch  19  Train_auc : 0.8119228068077483  , Valid_auc :  0.7262008565001083  ,& Test_auc :  0.7579151988133039  Avg Loss:  0.5930109686851501 Train Time (0m 53s) Eval Time (1m 16s)\n",
      "Epoch  20  Train_auc : 0.8087587331442968  , Valid_auc :  0.7357031995435984  ,& Test_auc :  0.7574777229834544  Avg Loss:  0.5908406283855439 Train Time (1m 33s) Eval Time (2m 10s)\n",
      "Epoch  21  Train_auc : 0.8123235762973291  , Valid_auc :  0.7316038076636582  ,& Test_auc :  0.7542167376695648  Avg Loss:  0.5890567748546601 Train Time (1m 31s) Eval Time (2m 8s)\n",
      "Epoch  22  Train_auc : 0.8107179647897458  , Valid_auc :  0.726123004177867  ,& Test_auc :  0.7479616117294587  Avg Loss:  0.5930958135128022 Train Time (1m 29s) Eval Time (2m 8s)\n",
      "Epoch  23  Train_auc : 0.8129792461778875  , Valid_auc :  0.7314144708159671  ,& Test_auc :  0.7501228357401251  Avg Loss:  0.5985468792915344 Train Time (1m 29s) Eval Time (2m 10s)\n",
      "Epoch  24  Train_auc : 0.8176037589763651  , Valid_auc :  0.7334386311942422  ,& Test_auc :  0.7498491408971089  Avg Loss:  0.5956624805927276 Train Time (1m 33s) Eval Time (2m 11s)\n",
      "Epoch  25  Train_auc : 0.8109126173793371  , Valid_auc :  0.7222428444373582  ,& Test_auc :  0.7476994376022464  Avg Loss:  0.5904996547698975 Train Time (1m 36s) Eval Time (2m 11s)\n",
      "Epoch  26  Train_auc : 0.8173821747581475  , Valid_auc :  0.7241449323743587  ,& Test_auc :  0.7485466772698768  Avg Loss:  0.5956561825275422 Train Time (1m 30s) Eval Time (2m 10s)\n",
      "Epoch  27  Train_auc : 0.8169460115854141  , Valid_auc :  0.728806106611593  ,& Test_auc :  0.7496592047716937  Avg Loss:  0.5899125504493713 Train Time (1m 34s) Eval Time (2m 10s)\n",
      "Epoch  28  Train_auc : 0.8075403113022646  , Valid_auc :  0.7186043383050864  ,& Test_auc :  0.7291865814175214  Avg Loss:  0.5906610531806946 Train Time (1m 28s) Eval Time (2m 14s)\n",
      "Epoch  29  Train_auc : 0.8194860467308094  , Valid_auc :  0.7278656505589174  ,& Test_auc :  0.7515820745134989  Avg Loss:  0.5849064271450043 Train Time (1m 32s) Eval Time (2m 9s)\n",
      "Epoch  30  Train_auc : 0.8185752737753329  , Valid_auc :  0.7249072623137461  ,& Test_auc :  0.7504058717040634  Avg Loss:  0.5915613796710967 Train Time (1m 31s) Eval Time (2m 13s)\n",
      "Epoch  31  Train_auc : 0.8208974743554144  , Valid_auc :  0.72089381939756  ,& Test_auc :  0.7512590274149444  Avg Loss:  0.5888978214263916 Train Time (1m 33s) Eval Time (2m 6s)\n",
      "Epoch  32  Train_auc : 0.8195928738353273  , Valid_auc :  0.7223101088437747  ,& Test_auc :  0.7504974146890995  Avg Loss:  0.586897616147995 Train Time (1m 34s) Eval Time (2m 14s)\n",
      "Epoch  33  Train_auc : 0.8137873977206164  , Valid_auc :  0.7240963525252803  ,& Test_auc :  0.7461547275724356  Avg Loss:  0.5980516791343689 Train Time (1m 30s) Eval Time (2m 9s)\n",
      "bestValidAuc 0.735703 has a TestAuc of 0.757478 at epoch 20 \n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "batch_size=100\n",
    "current_loss_l=[]\n",
    "all_losses_l=[]\n",
    "train_auc_allep =[]\n",
    "valid_auc_allep =[]\n",
    "test_auc_allep=[]\n",
    "#tests_auc_allep=[]\n",
    "#testl_auc_allep=[]\n",
    "bestValidAuc = 0.0\n",
    "bestTestAuc = 0.0\n",
    "bestValidEpoch = 0\n",
    "\n",
    "for ep in range(epochs):\n",
    "    \n",
    "    #print (model.embed.weight.data[13] )\n",
    "    start = time.time()\n",
    "    current_loss_la,all_losses_la = run_model_train(model,train_sl,batch_size)\n",
    "    train_time = timeSince(start)\n",
    "    eval_start = time.time()\n",
    "    train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "    test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "    #test_sh_auc = calculate_auc(model,test_sh_143,batch_size)\n",
    "    #test_l_auc = calculate_auc(model,test_l_143,batch_size)\n",
    "    valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "    eval_time = timeSince(eval_start)\n",
    "    all_losses_l.append (all_losses_la)\n",
    "    avg_loss = np.mean(all_losses_la)\n",
    "    train_auc_allep.append(train_auc)\n",
    "    valid_auc_allep.append(valid_auc)\n",
    "    test_auc_allep.append(test_auc)\n",
    "    #testl_auc_allep.append(test_l_auc)\n",
    "    #tests_auc_allep.append(test_sh_auc)\n",
    "    current_loss_l.append(current_loss_la)\n",
    "    print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "    #print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc,\"less than 5 visits history :\",test_sh_auc,\" for more :\",test_l_auc ,\" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "    \n",
    "    if valid_auc > bestValidAuc: \n",
    "        bestValidAuc = valid_auc\n",
    "        bestValidEpoch = ep\n",
    "        bestTestAuc = test_auc\n",
    "        best_model = model\n",
    "        torch.save(best_model, bmodel_pth)\n",
    "        torch.save(best_model.state_dict(), bmodel_st)\n",
    "    if ep - bestValidEpoch >12: break\n",
    "            \n",
    "print ('bestValidAuc %f has a TestAuc of %f at epoch %d ' % (bestValidAuc, bestTestAuc, bestValidEpoch))\n",
    "torch.save(best_model, bmodel_pth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~LailaRasmy/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('LailaRasmy','mzNHzVvwYjcZwBDZx3B7')\n",
    "\n",
    "train_auc_fg= go.Scatter(x= np.arange(epochs), y=train_auc_allep, name='train')\n",
    "test_auc_fg= go.Scatter(x= np.arange(epochs), y=test_auc_allep, name='test')\n",
    "#tests_auc_fg= go.Scatter(x= np.arange(epochs), y=tests_auc_allep, name='test<5')\n",
    "#testl_auc_fg= go.Scatter(x= np.arange(epochs), y=testl_auc_allep, name='test5+')\n",
    "\n",
    "valid_auc_fg= go.Scatter(x= np.arange(epochs), y=valid_auc_allep, name='valid')\n",
    "valid_max = max(valid_auc_allep)\n",
    "test_max = max(test_auc_allep)\n",
    "#data = [train_auc_fg,test_auc_fg,valid_auc_fg,tests_auc_fg,testl_auc_fg]#,valid_auc_allep,test_auc_allep] \n",
    "data = [train_auc_fg,test_auc_fg,valid_auc_fg]\n",
    "layout = go.Layout(xaxis=dict(dtick=1))\n",
    "layout.update(dict(annotations=[go.Annotation(text=\"Max Valid\", x=valid_auc_allep.index(valid_max), y=valid_max)]))\n",
    "#layout.update(dict(annotations=[go.Annotation(text=\"Max Test\", x=test_auc_allep.index(test_max), y=test_max)]))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='BiRNN_Auc')\n",
    "#url = py.plot(data, filename='some-data')  # gen. online plot\n",
    "#py.image.save_as(data, 'some-data.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
