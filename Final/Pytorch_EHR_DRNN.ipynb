{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is playing with pytorch framework for EHR modeling. In general, a patient's health record can be represented as a sequence of visits. Each visit has certain features, and can be represented as a list of medical codes.\n",
    "\n",
    "For simplicity, we are starting with the data structure that a patient's health record is a list of list, following the line of work from Jimeng Sun's lab. We will use codes from Ed Choi to manipulate the data. \n",
    "\n",
    "The core model is Dilated RNN as appears in https://arxiv.org/abs/1710.02224\n",
    "and using the code available on https://github.com/zalandoresearch/pt-dilate-rnn\n",
    "\n",
    "# todos:\n",
    "\n",
    "* save/load model\n",
    "* load pre-trained embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import sklearn \n",
    "from sklearn.metrics import roc_auc_score\n",
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "import sys, random\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "# for windows only    \n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data: load the input file containing list of list of list, and corresponding label file\n",
    "# and output the splitted training, valid and Test sets\n",
    "\n",
    "def data_load_split_VT(seqFile = 'data/cerner/hospital_data/h143.visits', labelFile = 'data/cerner/hospital_data/h143.labels' , test_r=0.2 , valid_r=0.1):\n",
    "\n",
    "    set_x = pickle.load(open(seqFile, 'rb'), encoding='bytes')\n",
    "    set_y = pickle.load(open(labelFile, 'rb'),encoding='bytes')\n",
    "    merged_set = [[set_y[i],set_x[i]] for i in range(len(set_x))] # merge the two lists\n",
    "\n",
    "    # set random seed\n",
    "    random.seed( 3 )\n",
    "    \n",
    "    dataSize = len(merged_set)\n",
    "    nTest = int(test_r * dataSize)\n",
    "    nValid = int(valid_r * dataSize)\n",
    "    \n",
    "    random.shuffle(merged_set)\n",
    "\n",
    "    test_set = merged_set[:nTest]\n",
    "    valid_set = merged_set[nTest:nTest+nValid]\n",
    "    train_set = merged_set[nTest+nValid:]\n",
    "\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sl , valid_sl , test_sl = data_load_split_VT()\n",
    "\n",
    "#print (train_sl[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_hidden, n_layers, dropout=0, cell_type='GRU'):\n",
    "\n",
    "        super(DRNN, self).__init__()\n",
    "\n",
    "        self.dilations = [2 ** i for i in range(n_layers)]\n",
    "        self.cell_type = cell_type\n",
    "        self.D = n_input\n",
    "        self.embedBag = nn.EmbeddingBag(20000,self.D ,mode= 'sum')\n",
    "\n",
    "        self.cells = nn.ModuleList([])\n",
    "\n",
    "        if self.cell_type == \"GRU\":\n",
    "            cell = nn.GRU\n",
    "        elif self.cell_type == \"RNN\":\n",
    "            cell = nn.RNN\n",
    "        elif self.cell_type == \"LSTM\":\n",
    "            cell = nn.LSTM\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                c = cell(n_input, n_hidden, dropout=dropout)\n",
    "            else:\n",
    "                c = cell(n_hidden, n_hidden, dropout=dropout)\n",
    "            self.cells.append(c)\n",
    "\n",
    "        self.out = nn.Linear(n_hidden,1)\n",
    "\n",
    "    \n",
    "\n",
    "    def EmbedPatient_MB(self, seq_mini_batch): # x is a ehr_seq_tensor\n",
    "        \n",
    "       \n",
    "        lp= len(max(seq_mini_batch, key=lambda xmb: len(xmb[1]))[1])\n",
    "        tb= torch.FloatTensor(len(seq_mini_batch),lp,self.D) \n",
    "        lbt1= torch.FloatTensor(len(seq_mini_batch),1)\n",
    "\n",
    "        for pt in range(len(seq_mini_batch)):\n",
    "              \n",
    "            lbt ,pt_visits =seq_mini_batch[pt]\n",
    "            lbt1[pt] = torch.FloatTensor([[float(lbt)]])\n",
    "            ml=(len(max(pt_visits, key=len))) ## getting the max number of visits for pts within the minibatch\n",
    "            txs= torch.LongTensor(len(pt_visits),ml)\n",
    "            \n",
    "            b=0\n",
    "            for i in pt_visits:\n",
    "                pd=(0, ml-len(i))\n",
    "                txs[b] = F.pad(torch.from_numpy(np.asarray(i)).view(1,-1),pd,\"constant\", 0).data\n",
    "                b=b+1\n",
    "            if use_cuda:\n",
    "                txs=txs.cuda()\n",
    "                \n",
    "            emb_bp= self.embedBag(Variable(txs)) ### embed will be num_of_visits*max_num_codes*embed_dim \n",
    "            #### the embed Bag dim will be num_of_visits*embed_dim\n",
    "            \n",
    "            #print ('embed bag Matrix : /n' , emb_bp )\n",
    "            \n",
    "            zp= nn.ZeroPad2d((0,0,0,(lp-len(pt_visits))))\n",
    "            xzp= zp(emb_bp)\n",
    "\n",
    "            #print ('padded embed bag Matrix : /n' , xzp )\n",
    "\n",
    "            tb[pt]=xzp.data\n",
    "        \n",
    "        #print ('pts embed bag Matrix : /n' , tb )\n",
    "        \n",
    "        # input for RNN need to be seq_len, batch, input_size\n",
    "        tb= tb.permute(1, 0, 2)  ### as my final input need to be seq_len x batch_size x input_size \n",
    "        \n",
    "        #print (tb.size())\n",
    "\n",
    "        #print ('Final input : /n' , tb )\n",
    "        \n",
    "        emb_m=Variable(tb)\n",
    "        label_tensor = Variable(lbt1)\n",
    "\n",
    "        if use_cuda:\n",
    "                label_tensor = label_tensor.cuda()\n",
    "                emb_m = emb_m.cuda()\n",
    "        \n",
    "        #print ('just for verificaton: /n Label tensor var: /n', label_tensor , 'input emb : /n', emb_m , 'input reformat done')\n",
    "        return emb_m , label_tensor\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \n",
    "        x , lt = self.EmbedPatient_MB(inputs)\n",
    "        outputs = []\n",
    "        for i, (cell, dilation) in enumerate(zip(self.cells, self.dilations)):\n",
    "            if hidden is None:\n",
    "                x = self.drnn_layer(cell, x, dilation)\n",
    "            else:\n",
    "                x = self.drnn_layer(cell, x, dilation, hidden[i])\n",
    "            \n",
    "            outputs.append(x[-dilation:])\n",
    "\n",
    "        #x= F.sigmoid(F.max_pool1d(self.out(x)))\n",
    "        #x = self.out(x).squeeze()\n",
    "        #print ('x dim', x.size())\n",
    "        x = F.sigmoid(F.max_pool1d(self.out(x).permute(2,1,0),x.size(0)))\n",
    "\n",
    "        return x.squeeze(), lt.squeeze()\n",
    "\n",
    "    def drnn_layer(self, cell, inputs, rate, hidden=None):\n",
    "\n",
    "        #n_steps = len(inputs)\n",
    "        n_steps = inputs.size(0)\n",
    "        #print('n_steps',n_steps) \n",
    "        #batch_size = inputs[0].size(0)\n",
    "        batch_size = inputs.size(1)\n",
    "        #print('batch size',batch_size) --verified correct\n",
    "        hidden_size = cell.hidden_size\n",
    "        #print('hidden size',hidden_size) --verified correct\n",
    "\n",
    "        inputs, dilated_steps = self._pad_inputs(inputs, n_steps, rate)\n",
    "        dilated_inputs = self._prepare_inputs(inputs, rate)\n",
    "\n",
    "        if hidden is None:\n",
    "            dilated_outputs = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size)\n",
    "        else:\n",
    "            hidden = self._prepare_inputs(hidden, rate)\n",
    "            dilated_outputs = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size, hidden=hidden)\n",
    "\n",
    "        splitted_outputs = self._split_outputs(dilated_outputs, rate)\n",
    "        outputs = self._unpad_outputs(splitted_outputs, n_steps)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "       \n",
    "    def _apply_cell(self, dilated_inputs, cell, batch_size, rate, hidden_size, hidden=None):\n",
    "\n",
    "        if hidden is None:\n",
    "            if self.cell_type == 'LSTM':\n",
    "                c, m = self.init_hidden(batch_size * rate, hidden_size)\n",
    "                hidden = (c.unsqueeze(0), m.unsqueeze(0))\n",
    "            else:\n",
    "                hidden = self.init_hidden(batch_size * rate, hidden_size).unsqueeze(0)\n",
    "\n",
    "        dilated_outputs = cell(dilated_inputs, hidden)[0]\n",
    "\n",
    "        return dilated_outputs\n",
    "\n",
    "    def _unpad_outputs(self, splitted_outputs, n_steps):\n",
    "\n",
    "        return splitted_outputs[:n_steps]\n",
    "\n",
    "    def _split_outputs(self, dilated_outputs, rate):\n",
    "\n",
    "        batchsize = dilated_outputs.size(1) // rate\n",
    "\n",
    "        blocks = [dilated_outputs[:, i * batchsize: (i + 1) * batchsize, :] for i in range(rate)]\n",
    "\n",
    "        interleaved = torch.stack((blocks)).transpose(1, 0).contiguous()\n",
    "        interleaved = interleaved.view(dilated_outputs.size(0) * rate,\n",
    "                                       batchsize,\n",
    "                                       dilated_outputs.size(2))\n",
    "        return interleaved\n",
    "\n",
    "    def _pad_inputs(self, inputs, n_steps, rate):\n",
    "\n",
    "        iseven = (n_steps % rate) == 0\n",
    "\n",
    "        if not iseven:\n",
    "            dilated_steps = n_steps // rate + 1\n",
    "\n",
    "            zeros_ = torch.zeros(dilated_steps * rate - inputs.size(0),\n",
    "                                 inputs.size(1),\n",
    "                                 inputs.size(2))\n",
    "            if use_cuda:\n",
    "                zeros_ = zeros_.cuda()\n",
    "\n",
    "            inputs = torch.cat((inputs, autograd.Variable(zeros_)))\n",
    "        else:\n",
    "            dilated_steps = n_steps // rate\n",
    "\n",
    "        return inputs, dilated_steps\n",
    "\n",
    "    def _prepare_inputs(self, inputs, rate):\n",
    "\n",
    "        dilated_inputs = torch.cat([inputs[j::rate, :, :] for j in range(rate)], 1)\n",
    "\n",
    "\n",
    "        return dilated_inputs\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden_dim):\n",
    "        c = autograd.Variable(torch.zeros(batch_size, hidden_dim))\n",
    "        if use_cuda:\n",
    "            c = c.cuda()\n",
    "        if self.cell_type == \"LSTM\":\n",
    "            m = autograd.Variable(torch.zeros(batch_size, hidden_dim))\n",
    "            if use_cuda:\n",
    "                m = m.cuda()\n",
    "            return (c, m)\n",
    "        else:\n",
    "            return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DRNN(128,64, 5, 0, 'GRU')\n",
    "if use_cuda:\n",
    "    model  = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (mini_batch, criterion, optimizer):  \n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    output , label_tensor = model (mini_batch)\n",
    "    \n",
    "\n",
    "    loss = criterion(output, label_tensor)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "   \n",
    "    return output, loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training all samples in random order\n",
    "import time\n",
    "import math\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_train(dataset,batch_size,learning_rate = 0.001 ):\n",
    "    \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #optimizer = optim.Adadelta(model.parameters(), '''lr=learning_rate,''' weight_decay=0)\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=learning_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), weight_decay=learning_rate)\n",
    "    #optimizer = optim.RMSprop (model.parameters(),lr=learning_rate,weight_decay=0.9)### same as reported in the paper , really poor\n",
    "    dataset.sort(key=lambda pt:len(pt[1])) \n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    print_every = 10#int(batch_size/2)\n",
    "    plot_every = 5\n",
    "    iter=0\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    #print('number of Batches',n_batches)\n",
    "    start = time.time()\n",
    "\n",
    "    for index in random.sample(range(n_batches), n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, loss = train(batch, criterion = nn.BCELoss(), optimizer = optimizer)\n",
    "            current_loss += loss\n",
    "            iter +=1\n",
    "            # Print iter number, loss, name and guess\n",
    "            #if iter % print_every == 0:\n",
    "               #print('%d %d%% (%s) %.4f ' % ( iter, iter/ n_batches * 100, timeSince(start), loss))\n",
    "\n",
    "            # Add current loss avg to list of losses\n",
    "            if iter % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "                \n",
    "    return current_loss,all_losses\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc(test_model, dataset, batch_size=200):\n",
    "\n",
    "    n_batches = int(np.ceil(int(len(dataset)) / int(batch_size)))\n",
    "    labelVec =[]\n",
    "    y_hat= []\n",
    "\n",
    "    for index in range(n_batches):\n",
    "            batch = dataset[index*batch_size:(index+1)*batch_size]\n",
    "            output, label_t = test_model(batch)\n",
    "            y_hat.extend(output.cpu().data.view(-1).numpy())\n",
    "            labelVec.extend(label_t.cpu().data.view(-1).numpy())\n",
    "    auc = roc_auc_score(labelVec, y_hat)\n",
    "    \n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "batch_size=200\n",
    "current_loss_l=[]\n",
    "all_losses_l=[]\n",
    "train_auc_allep =[]\n",
    "\n",
    "valid_auc_allep =[]\n",
    "\n",
    "test_auc_allep=[]\n",
    "\n",
    "\n",
    "for ep in range(epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    current_loss_la,all_losses_la = run_model_train(train_sl,batch_size)\n",
    "    train_time = timeSince(start)\n",
    "    eval_start = time.time()\n",
    "    train_auc = calculate_auc(model,train_sl,batch_size)\n",
    "    test_auc = calculate_auc(model,test_sl,batch_size)\n",
    "    valid_auc = calculate_auc(model,valid_sl,batch_size)\n",
    "    eval_time = timeSince(eval_start)\n",
    "    all_losses_l.append (all_losses_la)\n",
    "    avg_loss = np.mean(all_losses_la)\n",
    "    train_auc_allep.append(train_auc)\n",
    "    valid_auc_allep.append(valid_auc)\n",
    "    test_auc_allep.append(test_auc)\n",
    "    \n",
    "    print (\"Epoch \", ep,\" Train_auc :\", train_auc, \" , Valid_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc, \" Avg Loss: \", avg_loss, 'Train Time (%s) Eval Time (%s)'%(train_time,eval_time) )\n",
    "    \n",
    "    current_loss_l.append(current_loss_la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('LailaRasmy','mzNHzVvwYjcZwBDZx3B7')\n",
    "\n",
    "train_auc_fg= go.Scatter(x= np.arange(epochs), y=train_auc_allep, name='train')\n",
    "test_auc_fg= go.Scatter(x= np.arange(epochs), y=test_auc_allep, name='test')\n",
    "valid_auc_fg= go.Scatter(x= np.arange(epochs), y=valid_auc_allep, name='valid')\n",
    "valid_max = max(valid_auc_allep)\n",
    "test_max = max(test_auc_allep)\n",
    "data = [train_auc_fg,test_auc_fg,valid_auc_fg]#,valid_auc_allep,test_auc_allep] \n",
    "layout = go.Layout(xaxis=dict(dtick=1))\n",
    "layout.update(dict(annotations=[go.Annotation(text=\"Max Valid\", x=valid_auc_allep.index(valid_max), y=valid_max)]))\n",
    "#layout.update(dict(annotations=[go.Annotation(text=\"Max Test\", x=test_auc_allep.index(test_max), y=test_max)]))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='DRNN_Auc')\n",
    "#url = py.plot(data, filename='some-data')  # gen. online plot\n",
    "#py.image.save_as(data, 'some-data.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
